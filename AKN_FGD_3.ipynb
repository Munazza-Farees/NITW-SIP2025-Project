{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ZvnBTOFjLxyQICNCWVOsKLKtCz4qhO6m",
      "authorship_tag": "ABX9TyNjhmZhneT6Y3pZ4bzphtov",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Munazza-Farees/NITW-SIP2025-Project/blob/main/AKN_FGD_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "oTAzTDnNcPUt"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pywt\n",
        "from sklearn.metrics import classification_report, roc_curve, f1_score, silhouette_score, confusion_matrix\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from scipy.spatial.distance import euclidean, cdist\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import pearsonr\n",
        "from collections import Counter\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "# path = kagglehub.dataset_download(\"aikenkazin/ddos-sdn-dataset\")\n",
        "\n",
        "# print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "# URL = '/content/drive/MyDrive/Colab Notebooks/Client1 (copy).csv'\n",
        "# URL = '/content/drive/MyDrive/Colab Notebooks/Client2 (copy).csv'\n",
        "# URL = '/content/drive/MyDrive/Colab Notebooks/Combined datasets.csv'\n",
        "URL = '/content/drive/MyDrive/Colab Notebooks/Final_data.csv'\n",
        "data = pd.read_csv(URL)\n",
        "\n"
      ],
      "metadata": {
        "id": "rI8EzsZGcSmN"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing values\n",
        "data.fillna(data.select_dtypes(include=np.number).median(), inplace=True)\n",
        "data['Protocol'].fillna(data['Protocol'].mode()[0], inplace=True)\n",
        "data['Flags'].fillna(data['Flags'].mode()[0], inplace=True)\n",
        "\n",
        "# Convert types and encode categorical features\n",
        "data = data.astype({'Time': 'float', 'Label': int})\n",
        "data = pd.get_dummies(data, columns=['Protocol', 'Flags'], prefix=['Protocol', 'Flags'], dummy_na=False)\n",
        "data = data.drop(columns=['Source', 'Destination'], errors='ignore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "collapsed": true,
        "id": "n8TZgVqWcdZE",
        "outputId": "c6bfbfe1-203d-4e5a-eceb-f1c566ff0af0"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-173-476622721>:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['Protocol'].fillna(data['Protocol'].mode()[0], inplace=True)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Flags'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Flags'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-173-476622721>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Protocol'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Protocol'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Flags'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Flags'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Convert types and encode categorical features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Flags'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequence Alignment with Smith-Waterman Algorithm\n",
        "def smith_waterman(seq1, seq2, match_score=2, mismatch_score=-2, gap_penalty=-1):\n",
        "    n, m = len(seq1), len(seq2)\n",
        "    H = np.zeros((n+1, m+1))\n",
        "    for i in range(1, n+1):\n",
        "        for j in range(1, m+1):\n",
        "            match = H[i-1, j-1] + (match_score if seq1[i-1] == seq2[j-1] else mismatch_score)\n",
        "            delete = H[i-1, j] + gap_penalty\n",
        "            insert = H[i, j-1] + gap_penalty\n",
        "            H[i, j] = max(match, delete, insert, 0)\n",
        "    return np.max(H)\n",
        "\n",
        "# Construct benchmark sequences for RTO values\n",
        "rto_values = [1, 2]\n",
        "benchmark_sequences = {rto: np.array([1 if i % rto < 0.1 else 0 for i in np.arange(0, 60, 0.1)]) for rto in rto_values}"
      ],
      "metadata": {
        "id": "YjuQIQL2ciIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimate DU duration using UDP packet counts\n",
        "cw_duration = 60\n",
        "data['CW_ID'] = ((data['Time'] - data['Time'].min()) // cw_duration).astype(int)\n",
        "udp_data = data[data['Protocol_UDP'] == 1]\n",
        "cw_udp_counts = udp_data.groupby('CW_ID')['Length'].count().reindex(range(data['CW_ID'].max() + 1), fill_value=0)\n",
        "cw_udp_seq = cw_udp_counts.values\n",
        "\n",
        "max_score = -np.inf\n",
        "time_du = 1\n",
        "for rto, seq in benchmark_sequences.items():\n",
        "    score = smith_waterman(cw_udp_seq[:min(len(cw_udp_seq), len(seq))], seq[:min(len(cw_udp_seq), len(seq))])\n",
        "    if score > max_score:\n",
        "        max_score = score\n",
        "        time_du = rto\n",
        "\n",
        "# Create DUs based on estimated TimeDU\n",
        "data['DU_ID'] = ((data['Time'] - data['Time'].min()) // time_du).astype(int)\n",
        "print(\"Estimated DU Duration:\", time_du, \"seconds\")\n",
        "print(\"Packets per DU:\\n\", data.groupby('DU_ID').size().describe())"
      ],
      "metadata": {
        "id": "QA-YBAcfcmP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Aggregation\n",
        "data['TCP_Packets'] = data['Protocol_TCP'].astype(int)\n",
        "data['Total_Packets'] = 1\n",
        "data['Burstiness'] = data.groupby('DU_ID')['Packet_Rate'].transform(lambda x: x.max() / (x.mean() + 1e-10))\n",
        "\n",
        "features_to_aggregate = [\n",
        "    'Length', 'Inter_Arrival_Time', 'Connection_Duration', 'Packet_Rate',\n",
        "    'Flow_Bytes_Per_Second', 'Flow_Packets_Per_Second', 'Forward_Packets',\n",
        "    'Backward_Packets', 'Ratio_Fwd_Bwd', 'Entropy', 'Packet_Size_Variance',\n",
        "    'Burstiness', 'TCP_Packets', 'Total_Packets'\n",
        "]\n",
        "\n",
        "# Ensure features exist in dataset\n",
        "features_to_aggregate = [f for f in features_to_aggregate if f in data.columns]\n",
        "agg_funcs = {col: ['mean', 'std', 'max', 'min'] for col in features_to_aggregate}\n",
        "\n",
        "# Add explicit std-based features for:\n",
        "std_features = ['Length', 'Inter_Arrival_Time', 'Burstiness', 'Forward_Packets']\n",
        "for feature in std_features:\n",
        "    if feature in data.columns:\n",
        "        agg_funcs[feature] = agg_funcs.get(feature, []) + ['std']\n",
        "\n",
        "agg_funcs['Label'] = lambda x: 1 if (x == 1).mean() >= 0.05 else 0\n",
        "agg_data = data.groupby('DU_ID').agg(agg_funcs)\n",
        "agg_data.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in agg_data.columns]\n",
        "agg_data.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "ISgPUZJPcp-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute wavelet packet entropy per DU\n",
        "def compute_du_entropy(group):\n",
        "    signal = group['Length'].values\n",
        "    if len(signal) < 2:\n",
        "        return 0\n",
        "    try:\n",
        "        wp = pywt.WaveletPacket(data=signal, wavelet='db1', mode='symmetric', maxlevel=3)\n",
        "        energies = [np.sum(np.square(node.data)) for node in wp.get_level(3)]\n",
        "        total_energy = np.sum(energies) + 1e-10\n",
        "        probs = np.array(energies) / total_energy\n",
        "        return -np.sum(probs * np.log2(probs + 1e-10))\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "agg_data['Entropy_DU'] = data.groupby('DU_ID').apply(compute_du_entropy).reindex(agg_data.index).fillna(0).values"
      ],
      "metadata": {
        "id": "uIY6c4tPc2cM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Scaling\n",
        "agg_data_features = agg_data.drop(columns=['DU_ID', 'Label_<lambda>'])\n",
        "agg_data_features = agg_data_features.fillna(0)\n",
        "\n",
        "for col in agg_data_features.columns:\n",
        "    # Check if the column is a pandas Series before accessing dtype\n",
        "    col_data = agg_data_features[col]\n",
        "    if isinstance(col_data, pd.Series):\n",
        "        if col_data.dtype == bool:\n",
        "            agg_data_features[col] = col_data.astype(int)\n",
        "    else:\n",
        "        continue\n",
        "        # print(f\"Warning: agg_data_features[{col}] is not a Series. Type: {type(col_data)}\")\n",
        "\n",
        "\n",
        "for col in agg_data_features.columns:\n",
        "    p1, p99 = agg_data_features[col].quantile([0.01, 0.99])\n",
        "    col_data = agg_data_features[col]\n",
        "    if isinstance(col_data, pd.Series):\n",
        "        agg_data_features[col] = agg_data_features[col].clip(p1, p99)\n",
        "    else:\n",
        "        continue\n",
        "        # print(f\"Warning: agg_data_features[{col}] is not a Series. Type: {type(col_data)}\")\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(agg_data_features)\n",
        "X_scaled_data = pd.DataFrame(X_scaled, columns=agg_data_features.columns)\n",
        "X_scaled_data['DU_ID'] = agg_data['DU_ID'].values\n",
        "X_scaled_data['Label'] = agg_data['Label_<lambda>'].values\n",
        "X_scaled_data['Entropy_DU'] = agg_data['Entropy_DU'].values\n",
        "X_scaled_data = X_scaled_data.fillna(0)\n",
        "# print(X_scaled_data['TCP_Packets_mean'].value_counts())"
      ],
      "metadata": {
        "id": "xNlH9BWXc6ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Pearson correlation per DU\n",
        "pearson_coeffs = []\n",
        "for du_id in X_scaled_data['DU_ID'].unique():\n",
        "    du_data = data[data['DU_ID'] == du_id]\n",
        "    if len(du_data) > 1 and du_data['Total_Packets'].sum() > 0:\n",
        "        corr, _ = pearsonr(du_data['TCP_Packets'], du_data['Total_Packets'])\n",
        "        pearson_coeffs.append(corr if not np.isnan(corr) else 0)\n",
        "    else:\n",
        "        pearson_coeffs.append(0)\n",
        "pearson_map = dict(zip(X_scaled_data['DU_ID'].unique(), pearson_coeffs))\n",
        "X_scaled_data['Pearson_Corr'] = X_scaled_data['DU_ID'].map(pearson_map).fillna(0)"
      ],
      "metadata": {
        "id": "D0eHxMxec-ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AKN: Calculate Neurons' Number (Algorithm 1)\n",
        "def calculate_optimal_neurons(X, theta=0.5):\n",
        "    if len(X) < 2:\n",
        "        return 1, X\n",
        "    C = [X[0]]\n",
        "    second = X[1]\n",
        "    for i in range(2, len(X)):\n",
        "        if euclidean(X[i], C[0]) > euclidean(second, C[0]):\n",
        "            second = X[i]\n",
        "    C.append(second)\n",
        "\n",
        "    num_cl = 2\n",
        "\n",
        "    while True:\n",
        "        # Step 9–13: For each sample, compute distance to nearest center\n",
        "        D = np.min(cdist(X, np.array(C)), axis=1)\n",
        "        # Step 14: Find max of D\n",
        "        D_max = np.max(D)\n",
        "        id_D = np.argmax(D)\n",
        "        # Step 15: Check if D_max is large enough to add a new center\n",
        "\n",
        "        dist_C1C2 = euclidean(C[1], C[0])\n",
        "        if D_max > theta * dist_C1C2:\n",
        "            C.append(X[id_D])\n",
        "            num_cl += 1\n",
        "        else:\n",
        "            break\n",
        "    return num_cl, np.array(C)"
      ],
      "metadata": {
        "id": "lMGRsMK5dClp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AKN: Calculate Initial Weights (Algorithm 2)\n",
        "def calculate_initial_weights(X_raw, num_cl):\n",
        "    R = np.array(X_raw)\n",
        "    n, m = R.shape\n",
        "\n",
        "    # Step 1–5: Normalize R into N\n",
        "    N = np.zeros_like(R)\n",
        "    for j in range(m):\n",
        "        col = R[:, j]\n",
        "        col_min = col.min()\n",
        "        col_max = col.max()\n",
        "        if col_max - col_min == 0:\n",
        "            N[:, j] = 0\n",
        "        else:\n",
        "            N[:, j] = (col - col_min) / (col_max - col_min)\n",
        "\n",
        "    # Step 6: Calculate Center Vector (mean of N)\n",
        "    C = np.mean(N, axis=0)\n",
        "\n",
        "    # Step 7: Calculate d_max = max distance from any point to center\n",
        "    distances = cdist(N, [C])\n",
        "    d_max = np.max(distances)\n",
        "\n",
        "    # Step 8–12: Initialize weights\n",
        "    np.random.seed(42)  # for reproducibility\n",
        "    X_init = np.zeros((num_cl, m))\n",
        "\n",
        "    for j in range(num_cl):\n",
        "        # For each weight, add or subtract a small random delta from C\n",
        "        random_shifts = (np.random.rand(m) - 0.5) * 2 * d_max  # in range [-d_max, +d_max]\n",
        "        X_init[j] = C + random_shifts\n",
        "\n",
        "    return X_init"
      ],
      "metadata": {
        "id": "2k2Is3sadFhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kohonen_clustering(X, weights, learning_radius=0.3, initial_eta=0.5, num_epochs=100, sigma=1.0):\n",
        "    n, m = X.shape\n",
        "    num_cl = weights.shape[0]\n",
        "    w = weights.copy()\n",
        "\n",
        "    for t in range(1, num_epochs + 1):\n",
        "        for i in range(n):\n",
        "            x = X[i]\n",
        "            distances = np.linalg.norm(w - x, axis=1)\n",
        "            winner_idx = np.argmin(distances)\n",
        "            winner = w[winner_idx]\n",
        "\n",
        "            neighbor_mask = np.linalg.norm(X - winner, axis=1) < learning_radius\n",
        "            neighbors = X[neighbor_mask]\n",
        "\n",
        "            if len(neighbors) == 0:\n",
        "                continue\n",
        "\n",
        "            S = np.linalg.norm(neighbors - winner, axis=1)\n",
        "            T = np.exp(-np.square(S) / (2 * sigma ** 2))\n",
        "            eta_t = initial_eta * np.exp(-t / num_epochs)\n",
        "\n",
        "            for k, neighbor in enumerate(neighbors):\n",
        "                neuron_idx = np.random.randint(0, num_cl)\n",
        "                influence = T[k]\n",
        "                delta = eta_t * influence * (neighbor - w[neuron_idx])\n",
        "                w[neuron_idx] += delta\n",
        "\n",
        "    cluster_indices = np.argmin(cdist(X, w), axis=1)\n",
        "    return cluster_indices, w"
      ],
      "metadata": {
        "id": "BdzbRuU1dIJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for clustering\n",
        "feature_cols = [col for col in X_scaled_data.columns if col not in ['DU_ID', 'Label']]\n",
        "X_clustering = X_scaled_data[feature_cols].values\n",
        "\n",
        "# Step 1: Calculate optimal neurons\n",
        "num_neurons, initial_centers = calculate_optimal_neurons(X_clustering, theta=0.3)\n",
        "\n",
        "# Step 2: Initialize weights\n",
        "initial_weights = calculate_initial_weights(X_clustering, num_neurons)\n",
        "initial_weights = initial_centers.copy()\n",
        "\n",
        "# Step 3: Apply AKN clustering\n",
        "cluster_labels, final_weights = kohonen_clustering(\n",
        "    X_clustering, initial_weights, num_epochs=100,\n",
        "    learning_radius=0.1, initial_eta=0.5\n",
        ")\n",
        "\n",
        "# Save results\n",
        "X_scaled_data['Cluster'] = cluster_labels\n",
        "\n",
        "# Evaluate clustering\n",
        "if len(set(cluster_labels)) > 1:\n",
        "    print(\"✅ Clustering Silhouette Score:\", silhouette_score(X_clustering, cluster_labels))\n",
        "else:\n",
        "    print(\"⚠️ Only one cluster formed — check theta or neuron spread.\")\n"
      ],
      "metadata": {
        "id": "zFunwpLGdN5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X_tsne = TSNE(n_components=2, random_state=42).fit_transform(X_clustering)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.title(\"t-SNE of DU Features\")\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=X_scaled_data['Label'], cmap='coolwarm', alpha=0.7)\n",
        "plt.xlabel(\"TSNE-1\")\n",
        "plt.ylabel(\"TSNE-2\")\n",
        "plt.colorbar(label=\"Label (0 = Normal, 1 = Attack)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XqXJFzKBq023"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Cross tab: Cluster vs True Label\n",
        "cluster_purity = pd.crosstab(X_scaled_data['Cluster'], X_scaled_data['Label'])\n",
        "print(cluster_purity)\n",
        "\n",
        "# Optional: Normalize row-wise to see %\n",
        "cluster_purity_norm = cluster_purity.div(cluster_purity.sum(axis=1), axis=0)\n",
        "sns.heatmap(cluster_purity_norm, annot=True, cmap='coolwarm')\n",
        "plt.title(\"Cluster Purity by Label\")\n",
        "plt.xlabel(\"True Label\")\n",
        "plt.ylabel(\"Cluster\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q8KEq1NjrCGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "kmeans = KMeans(n_clusters=num_neurons, random_state=42).fit(X_clustering)\n",
        "X_scaled_data['Cluster_KMeans'] = kmeans.labels_\n",
        "\n",
        "score = silhouette_score(X_clustering, kmeans.labels_)\n",
        "print(f\"KMeans Silhouette Score: {score:.4f}\")"
      ],
      "metadata": {
        "id": "91rF8o28rIsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute MAD on original data\n",
        "mean_C = (X_scaled_data['TCP_Packets_mean'] / (X_scaled_data['TCP_Packets_mean'] + 1e-10)).mean()\n",
        "std_C = (X_scaled_data['TCP_Packets_mean'] / (X_scaled_data['TCP_Packets_mean'] + 1e-10)).std() or 1e-10\n",
        "\n",
        "mean_H = X_scaled_data['Entropy_DU'].mean()\n",
        "std_H = X_scaled_data['Entropy_DU'].std() or 1e-10\n",
        "\n",
        "mean_P = X_scaled_data['Pearson_Corr'].mean()\n",
        "std_P = X_scaled_data['Pearson_Corr'].std() or 1e-10\n",
        "\n",
        "mean_B = X_scaled_data['Burstiness_mean'].mean()\n",
        "std_B = X_scaled_data['Burstiness_mean'].std()\n",
        "\n",
        "\n",
        "mad_scores = []\n",
        "for _, row in X_scaled_data.iterrows():\n",
        "    # C = (row['TCP_Packets_mean'] / (row['TCP_Packets_mean'] + 1e-10) - mean_C) / std_C\n",
        "    C = (row['Burstiness_mean'] - mean_B) / std_B\n",
        "    H = (row['Entropy_DU'] - mean_H) / std_H\n",
        "    P = (row['Pearson_Corr'] - mean_P) / std_P\n",
        "    mad = np.sqrt(0.4 * C**2 + 0.3 * H**2 + 0.3 * P**2)\n",
        "    # mad = (np.abs(C) + np.abs(H) + np.abs(P)) / 3\n",
        "    mad_scores.append(mad if not np.isnan(mad) else 0)\n",
        "X_scaled_data['MAD'] = mad_scores\n",
        "X_scaled_data['MAD'] = scaler.fit_transform(X_scaled_data[['MAD']]).flatten()\n",
        "X_scaled_data['MAD'] = X_scaled_data['MAD'].fillna(0)"
      ],
      "metadata": {
        "id": "4Tf2ewYBdQ-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Balance data for threshold optimization\n",
        "# X_array = X_scaled_data.drop(columns=['DU_ID', 'Label', 'Entropy_DU', 'Pearson_Corr', 'Cluster', 'MAD']).values\n",
        "# y_array = X_scaled_data['Label'].values\n",
        "# try:\n",
        "#     smote = SMOTE(random_state=42, k_neighbors=min(5, max(1, len(y_array[y_array == 1]) - 1)))\n",
        "#     X_resampled, y_resampled = smote.fit_resample(X_array, y_array)\n",
        "# except ValueError:\n",
        "#     X_resampled, y_resampled = X_array, y_array\n",
        "# X_scaled_data_resampled = pd.DataFrame(X_resampled, columns=X_scaled_data.drop(columns=['DU_ID', 'Label', 'Entropy_DU', 'Pearson_Corr', 'Cluster', 'MAD']).columns)\n",
        "# X_scaled_data_resampled['Label'] = y_resampled\n",
        "# # Ensure MAD values align with resampled data length\n",
        "# mad_values = X_scaled_data['MAD'].values\n",
        "# if len(X_resampled) > len(mad_values):\n",
        "#     # Pad with mean MAD if resampled data is longer\n",
        "#     mad_extended = np.pad(mad_values, (0, len(X_resampled) - len(mad_values)), mode='constant', constant_values=mad_values.mean())\n",
        "# else:\n",
        "#     mad_extended = mad_values[:len(X_resampled)]\n",
        "# X_scaled_data_resampled['MAD'] = scaler.transform(mad_extended.reshape(-1, 1)).flatten()"
      ],
      "metadata": {
        "id": "lNco9qtpe2ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimize Thresholds\n",
        "fpr, tpr, thresholds = roc_curve(X_scaled_data['Label'], X_scaled_data['MAD'])\n",
        "optimal_idx = np.argmax(tpr - fpr)\n",
        "optimal_z = (thresholds[optimal_idx] - X_scaled_data['MAD'].mean()) / (X_scaled_data['MAD'].std() + 1e-10)\n",
        "# LMAD = X_scaled_data_resampled['MAD'].mean() + optimal_z * X_scaled_data_resampled['MAD'].std()\n",
        "LMAD = np.percentile(X_scaled_data['MAD'], 90)\n",
        "X_scaled_data['Abnormal_DU'] = (X_scaled_data['MAD'] > LMAD).astype(int)\n",
        "\n",
        "best_ladur = 0.55\n",
        "best_f1 = 0\n",
        "for ladur in [0.5, 0.55, 0.6]:\n",
        "    cluster_votes = X_scaled_data.groupby('Cluster')['Abnormal_DU'].mean()\n",
        "    abnormal_clusters = cluster_votes[cluster_votes >= ladur].index\n",
        "    X_scaled_data['Predicted_Label'] = X_scaled_data['Cluster'].apply(lambda c: 1 if c in abnormal_clusters else 0)\n",
        "    f1 = f1_score(X_scaled_data['Label'], X_scaled_data['Predicted_Label'])\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_ladur = ladur"
      ],
      "metadata": {
        "id": "dkCRko53dbnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Detection\n",
        "LMAD = X_scaled_data['MAD'].mean() + optimal_z * X_scaled_data['MAD'].std()\n",
        "\n",
        "X_scaled_data['Abnormal_DU'] = (X_scaled_data['MAD'] > LMAD).astype(int)\n",
        "\n",
        "cluster_votes = X_scaled_data.groupby('Cluster')['Abnormal_DU'].mean()\n",
        "cluster_to_label = X_scaled_data.groupby('Cluster')['Label'].agg(lambda x: x.mode()[0])\n",
        "X_scaled_data['Predicted_Label'] = X_scaled_data['Cluster'].map(cluster_to_label)\n",
        "\n",
        "abnormal_clusters = cluster_votes[cluster_votes >= best_ladur].index\n",
        "\n",
        "X_scaled_data['Predicted_Label'] = X_scaled_data['Cluster'].apply(lambda c: 1 if c in abnormal_clusters else 0)\n",
        "\n",
        "X_scaled_data['Predicted_Label'] = X_scaled_data['Predicted_Label'].fillna(0)\n",
        "\n",
        "# print(\"Confusion matrix:\\n\")\n",
        "# print(confusion_matrix(X_scaled_data['Label'], X_scaled_data['Predicted_Label']))\n",
        "\n",
        "print(\"Final Detection Evaluation:\\n\")\n",
        "print(classification_report(X_scaled_data['Label'], X_scaled_data['Predicted_Label']))"
      ],
      "metadata": {
        "id": "Fw4xDL2Rdhhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validate Random Forest\n",
        "# rf = RandomForestClassifier(random_state=42)\n",
        "# scores = cross_val_score(rf, X_array, X_scaled_data['Label'], cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), scoring='f1_weighted')\n",
        "# print(\"Random Forest Cross-Validation F1 Scores:\", scores, \"Mean:\", scores.mean())\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "X_array = X_scaled_data.drop(columns=['DU_ID', 'Label', 'Entropy_DU', 'Pearson_Corr', 'Cluster', 'MAD']).values\n",
        "y_array = X_scaled_data['Label'].values\n",
        "rf.fit(X_array, y_array)\n",
        "y_pred_rf = rf.predict(X_array)\n",
        "print(\"Random Forest Classification Report:\\n\")\n",
        "print(classification_report(y_array, y_pred_rf))"
      ],
      "metadata": {
        "id": "9WyS4l5jdkHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "# # Feature Selection using Random Forest\n",
        "# rf = RandomForestClassifier(random_state=42)\n",
        "# X_array = X_scaled_data.drop(columns=['DU_ID', 'Label', 'Entropy_DU', 'Pearson_Corr', 'Cluster', 'MAD']).values\n",
        "# y_array = X_scaled_data['Label'].values\n",
        "# rf.fit(X_array, y_array)\n",
        "# feature_importances = pd.Series(rf.feature_importances_, index=X_scaled_data.drop(columns=['DU_ID', 'Label', 'Entropy_DU', 'Pearson_Corr', 'Cluster', 'MAD']).columns)\n",
        "# top_features = feature_importances.nlargest(20).index\n",
        "# X_array = X_scaled_data[top_features].values\n",
        "# print(\"Top 20 Features:\\n\", top_features)\n",
        "\n",
        "# # Split data for proper evaluation\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_array, y_array, test_size=0.2, stratify=y_array, random_state=42)\n",
        "\n",
        "# # Balance training data\n",
        "# try:\n",
        "#     smote = SMOTE(random_state=42, k_neighbors=min(5, max(1, len(y_train[y_train == 1]) - 1)))\n",
        "#     X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "# except ValueError:\n",
        "#     X_train_resampled, y_train_resampled = X_train, y_train\n",
        "\n",
        "# # Optimize Thresholds for AKN-FGD\n",
        "# X_scaled_data_resampled = pd.DataFrame(X_train_resampled, columns=top_features)\n",
        "# X_scaled_data_resampled['Label'] = y_train_resampled\n",
        "# mad_values = X_scaled_data['MAD'].values\n",
        "# if len(X_train_resampled) > len(mad_values):\n",
        "#     mad_extended = np.pad(mad_values, (0, len(X_train_resampled) - len(mad_values)), mode='constant', constant_values=mad_values.mean())\n",
        "# else:\n",
        "#     mad_extended = mad_values[:len(X_train_resampled)]\n",
        "# X_scaled_data_resampled['MAD'] = scaler.transform(mad_extended.reshape(-1, 1)).flatten()\n",
        "# # Extend or truncate Cluster values to match resampled data length\n",
        "# cluster_values = X_scaled_data['Cluster'].values\n",
        "# if len(X_train_resampled) > len(cluster_values):\n",
        "#     cluster_extended = np.pad(cluster_values, (0, len(X_train_resampled) - len(cluster_values)), mode='constant', constant_values=np.median(cluster_values))\n",
        "# else:\n",
        "#     cluster_extended = cluster_values[:len(X_train_resampled)]\n",
        "# X_scaled_data_resampled['Cluster'] = cluster_extended\n",
        "\n",
        "# fpr, tpr, thresholds = roc_curve(X_scaled_data_resampled['Label'], X_scaled_data_resampled['MAD'])\n",
        "# optimal_idx = np.argmax(tpr - fpr)\n",
        "# optimal_z = (thresholds[optimal_idx] - X_scaled_data_resampled['MAD'].mean()) / (X_scaled_data_resampled['MAD'].std() + 1e-10)\n",
        "# LMAD = X_scaled_data_resampled['MAD'].mean() + optimal_z * X_scaled_data_resampled['MAD'].std()\n",
        "# X_scaled_data['Abnormal_DU'] = (X_scaled_data['MAD'] > LMAD).astype(int)\n",
        "\n",
        "# best_ladur = 0.55\n",
        "# best_f1 = 0\n",
        "# for ladur in [0.3, 0.4, 0.5, 0.55, 0.6, 0.7]:\n",
        "#     cluster_votes = X_scaled_data.groupby('Cluster')['Abnormal_DU'].mean()\n",
        "#     abnormal_clusters = cluster_votes[cluster_votes >= ladur].index\n",
        "#     X_scaled_data['Predicted_Label'] = X_scaled_data['Cluster'].apply(lambda c: 1 if c in abnormal_clusters else 0)\n",
        "#     f1 = f1_score(X_scaled_data['Label'], X_scaled_data['Predicted_Label'])\n",
        "#     if f1 > best_f1:\n",
        "#         best_f1 = f1\n",
        "#         best_ladur = ladur\n",
        "\n",
        "# # Final Detection for AKN-FGD\n",
        "# LMAD = X_scaled_data['MAD'].mean() + optimal_z * X_scaled_data['MAD'].std()\n",
        "# X_scaled_data['Abnormal_DU'] = (X_scaled_data['MAD'] > LMAD).astype(int)\n",
        "# cluster_votes = X_scaled_data.groupby('Cluster')['Abnormal_DU'].mean()\n",
        "# abnormal_clusters = cluster_votes[cluster_votes >= best_ladur].index\n",
        "# X_scaled_data['Predicted_Label'] = X_scaled_data['Cluster'].apply(lambda c: 1 if c in abnormal_clusters else 0)\n",
        "# X_scaled_data['Predicted_Label'] = X_scaled_data['Predicted_Label'].fillna(0)\n",
        "# print(\"Final Detection Evaluation (AKN-FGD):\\n\")\n",
        "# print(classification_report(X_scaled_data['Label'], X_scaled_data['Predicted_Label']))\n",
        "\n",
        "# # Random Forest Evaluation with Cross-Validation\n",
        "# rf = RandomForestClassifier(max_depth=10, min_samples_split=5, random_state=42)\n",
        "# scores = cross_val_score(rf, X_train, y_train, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), scoring='f1_weighted')\n",
        "# print(\"Random Forest Cross-Validation F1 Scores:\", scores, \"Mean:\", scores.mean())\n",
        "# rf.fit(X_train, y_train)\n",
        "# y_pred_rf = rf.predict(X_test)\n",
        "# print(\"Random Forest Test Set Classification Report:\\n\")\n",
        "# print(classification_report(y_test, y_pred_rf))"
      ],
      "metadata": {
        "id": "oUoyVLUMi2hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Lightweight Neural Network\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense, Dropout\n",
        "# from tensorflow.keras.callbacks import EarlyStopping\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# model = Sequential([\n",
        "#     Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "#     Dropout(0.2),\n",
        "#     Dense(16, activation='relu'),\n",
        "#     Dropout(0.2),\n",
        "#     Dense(1, activation='sigmoid')\n",
        "# ])\n",
        "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "# history = model.fit(X_train_resampled, y_train_resampled, validation_split=0.2, epochs=50, batch_size=32, callbacks=[early_stopping], verbose=0)\n",
        "# y_pred_nn = (model.predict(X_test) > 0.5).astype(int)\n",
        "# print(\"Neural Network Test Set Classification Report:\\n\")\n",
        "# print(classification_report(y_test, y_pred_nn))"
      ],
      "metadata": {
        "id": "AlcVoLh6kLXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Balance data for threshold optimization\n",
        "# X_array = X_scaled_data.drop(columns=['DU_ID', 'Label', 'Entropy_DU', 'Pearson_Corr', 'Cluster', 'MAD']).values\n",
        "# y_array = X_scaled_data['Label'].values\n",
        "# try:\n",
        "#     smote = SMOTE(random_state=42, k_neighbors=min(5, len(y_array[y_array == 1]) - 1))\n",
        "#     X_resampled, y_resampled = smote.fit_resample(X_array, y_array)\n",
        "# except ValueError:\n",
        "#     X_resampled, y_resampled = X_array, y_array\n",
        "# X_scaled_data_resampled = pd.DataFrame(X_resampled, columns=X_scaled_data.drop(columns=['DU_ID', 'Label', 'Entropy_DU', 'Pearson_Corr', 'Cluster', 'MAD']).columns)\n",
        "# X_scaled_data_resampled['Label'] = y_resampled\n",
        "# X_scaled_data_resampled['MAD'] = scaler.transform(X_scaled_data[['MAD']].iloc[:len(X_resampled)].values).flatten()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kNIRu1yNdVfF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}