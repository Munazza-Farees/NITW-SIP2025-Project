{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ZvnBTOFjLxyQICNCWVOsKLKtCz4qhO6m",
      "authorship_tag": "ABX9TyN31fUbWjt0csr6zLIs6uHL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Munazza-Farees/NITW-SIP2025-Project/blob/main/AKN_FGD_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "oTAzTDnNcPUt"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pywt\n",
        "from sklearn.metrics import classification_report, roc_curve, f1_score, silhouette_score\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from scipy.spatial.distance import euclidean\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import pearsonr\n",
        "from collections import Counter\n",
        "from sklearn.neighbors import NearestNeighbors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "URL = '/content/drive/MyDrive/Colab Notebooks/Client1 (copy).csv'\n",
        "# URL = '/content/drive/MyDrive/Colab Notebooks/Client2 (copy).csv'\n",
        "data = pd.read_csv(URL)\n",
        "\n"
      ],
      "metadata": {
        "id": "rI8EzsZGcSmN"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing values\n",
        "data.fillna(data.select_dtypes(include=np.number).median(), inplace=True)\n",
        "data['Protocol'].fillna(data['Protocol'].mode()[0], inplace=True)\n",
        "data['Flags'].fillna(data['Flags'].mode()[0], inplace=True)\n",
        "\n",
        "# Convert types and encode categorical features\n",
        "data = data.astype({'Time': 'float', 'Label': int})\n",
        "data = pd.get_dummies(data, columns=['Protocol', 'Flags'], prefix=['Protocol', 'Flags'], dummy_na=False)\n",
        "data = data.drop(columns=['Source', 'Destination'], errors='ignore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "n8TZgVqWcdZE",
        "outputId": "a2cf24bf-0332-415d-deaf-d309e0898ef2"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-81-476622721>:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['Protocol'].fillna(data['Protocol'].mode()[0], inplace=True)\n",
            "<ipython-input-81-476622721>:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['Flags'].fillna(data['Flags'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequence Alignment with Smith-Waterman Algorithm\n",
        "def smith_waterman(seq1, seq2, match_score=2, mismatch_score=-2, gap_penalty=-1):\n",
        "    n, m = len(seq1), len(seq2)\n",
        "    H = np.zeros((n+1, m+1))\n",
        "    for i in range(1, n+1):\n",
        "        for j in range(1, m+1):\n",
        "            match = H[i-1, j-1] + (match_score if seq1[i-1] == seq2[j-1] else mismatch_score)\n",
        "            delete = H[i-1, j] + gap_penalty\n",
        "            insert = H[i, j-1] + gap_penalty\n",
        "            H[i, j] = max(match, delete, insert, 0)\n",
        "    return np.max(H)\n",
        "\n",
        "# Construct benchmark sequences for RTO values\n",
        "rto_values = [1, 2]\n",
        "benchmark_sequences = {rto: np.array([1 if i % rto < 0.1 else 0 for i in np.arange(0, 60, 0.1)]) for rto in rto_values}"
      ],
      "metadata": {
        "id": "YjuQIQL2ciIH"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimate DU duration using UDP packet counts\n",
        "cw_duration = 60\n",
        "data['CW_ID'] = ((data['Time'] - data['Time'].min()) // cw_duration).astype(int)\n",
        "udp_data = data[data['Protocol_UDP'] == 1]\n",
        "cw_udp_counts = udp_data.groupby('CW_ID')['Length'].count().reindex(range(data['CW_ID'].max() + 1), fill_value=0)\n",
        "cw_udp_seq = cw_udp_counts.values\n",
        "\n",
        "max_score = -np.inf\n",
        "time_du = 1\n",
        "for rto, seq in benchmark_sequences.items():\n",
        "    score = smith_waterman(cw_udp_seq[:min(len(cw_udp_seq), len(seq))], seq[:min(len(cw_udp_seq), len(seq))])\n",
        "    if score > max_score:\n",
        "        max_score = score\n",
        "        time_du = rto\n",
        "\n",
        "# Create DUs based on estimated TimeDU\n",
        "data['DU_ID'] = ((data['Time'] - data['Time'].min()) // time_du).astype(int)\n",
        "print(\"Estimated DU Duration:\", time_du, \"seconds\")\n",
        "print(\"Packets per DU:\\n\", data.groupby('DU_ID').size().describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QA-YBAcfcmP_",
        "outputId": "3d51f3f7-74cd-4ad4-afe3-638a71d6a052"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated DU Duration: 2 seconds\n",
            "Packets per DU:\n",
            " count    733.000000\n",
            "mean      47.553888\n",
            "std      131.489508\n",
            "min        2.000000\n",
            "25%        8.000000\n",
            "50%       14.000000\n",
            "75%       24.000000\n",
            "max      733.000000\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Aggregation\n",
        "data['TCP_Packets'] = data['Protocol_TCP'].astype(int)\n",
        "data['Total_Packets'] = 1\n",
        "data['Burstiness'] = data.groupby('DU_ID')['Packet_Rate'].transform(lambda x: x.max() / (x.mean() + 1e-10))\n",
        "\n",
        "features_to_aggregate = [\n",
        "    'Length', 'Inter_Arrival_Time', 'Connection_Duration', 'Packet_Rate',\n",
        "    'Flow_Bytes_Per_Second', 'Flow_Packets_Per_Second', 'Forward_Packets',\n",
        "    'Backward_Packets', 'Ratio_Fwd_Bwd', 'Entropy', 'Packet_Size_Variance',\n",
        "    'Burstiness', 'TCP_Packets', 'Total_Packets'\n",
        "]\n",
        "\n",
        "# Ensure features exist in dataset\n",
        "features_to_aggregate = [f for f in features_to_aggregate if f in data.columns]\n",
        "agg_funcs = {col: ['mean', 'std', 'max', 'min'] for col in features_to_aggregate}\n",
        "agg_funcs['Label'] = lambda x: 1 if (x == 1).mean() >= 0.05 else 0\n",
        "agg_data = data.groupby('DU_ID').agg(agg_funcs)\n",
        "agg_data.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in agg_data.columns]\n",
        "agg_data.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "ISgPUZJPcp-Y"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute wavelet packet entropy per DU\n",
        "def compute_du_entropy(group):\n",
        "    signal = group['Length'].values\n",
        "    if len(signal) < 2:\n",
        "        return 0\n",
        "    try:\n",
        "        wp = pywt.WaveletPacket(data=signal, wavelet='db1', mode='symmetric', maxlevel=3)\n",
        "        energies = [np.sum(np.square(node.data)) for node in wp.get_level(3)]\n",
        "        total_energy = np.sum(energies) + 1e-10\n",
        "        probs = np.array(energies) / total_energy\n",
        "        return -np.sum(probs * np.log2(probs + 1e-10))\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "agg_data['Entropy_DU'] = data.groupby('DU_ID').apply(compute_du_entropy).reindex(agg_data.index).fillna(0).values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIY6c4tPc2cM",
        "outputId": "af4db422-98d9-445d-d14e-af15cd3d127c"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-85-3317536019>:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  agg_data['Entropy_DU'] = data.groupby('DU_ID').apply(compute_du_entropy).reindex(agg_data.index).fillna(0).values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Scaling\n",
        "agg_data_features = agg_data.drop(columns=['DU_ID', 'Label_<lambda>'])\n",
        "agg_data_features = agg_data_features.fillna(0)\n",
        "\n",
        "for col in agg_data_features.columns:\n",
        "    if agg_data_features[col].dtype == bool:\n",
        "        agg_data_features[col] = agg_data_features[col].astype(int)\n",
        "\n",
        "for col in agg_data_features.columns:\n",
        "    p1, p99 = agg_data_features[col].quantile([0.01, 0.99])\n",
        "    agg_data_features[col] = agg_data_features[col].clip(lower=p1, upper=p99)\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(agg_data_features)\n",
        "X_scaled_data = pd.DataFrame(X_scaled, columns=agg_data_features.columns)\n",
        "X_scaled_data['DU_ID'] = agg_data['DU_ID'].values\n",
        "X_scaled_data['Label'] = agg_data['Label_<lambda>'].values\n",
        "X_scaled_data['Entropy_DU'] = agg_data['Entropy_DU'].values\n",
        "X_scaled_data = X_scaled_data.fillna(0)"
      ],
      "metadata": {
        "id": "xNlH9BWXc6ly"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Pearson correlation per DU\n",
        "pearson_coeffs = []\n",
        "for du_id in X_scaled_data['DU_ID'].unique():\n",
        "    du_data = data[data['DU_ID'] == du_id]\n",
        "    if len(du_data) > 1 and du_data['Total_Packets'].sum() > 0:\n",
        "        corr, _ = pearsonr(du_data['TCP_Packets'], du_data['Total_Packets'])\n",
        "        pearson_coeffs.append(corr if not np.isnan(corr) else 0)\n",
        "    else:\n",
        "        pearson_coeffs.append(0)\n",
        "pearson_map = dict(zip(X_scaled_data['DU_ID'].unique(), pearson_coeffs))\n",
        "X_scaled_data['Pearson_Corr'] = X_scaled_data['DU_ID'].map(pearson_map).fillna(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0eHxMxec-ht",
        "outputId": "95dcb79b-59f6-4c32-de42-100a655dcb3d"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-87-1285548057>:6: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  corr, _ = pearsonr(du_data['TCP_Packets'], du_data['Total_Packets'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AKN: Calculate Neurons' Number (Algorithm 1)\n",
        "def calculate_optimal_neurons(X, threshold_ratio=0.5):\n",
        "    if len(X) < 2:\n",
        "        return 1, X\n",
        "    centers = [X[0]]\n",
        "    max_dist = 0\n",
        "    second = X[1]\n",
        "    for row in X[1:]:\n",
        "        dist = euclidean(row, centers[0])\n",
        "        if dist > max_dist:\n",
        "            max_dist = dist\n",
        "            second = row\n",
        "    centers.append(second)\n",
        "    while True:\n",
        "        min_dists = [min([euclidean(row, center) for center in centers]) for row in X]\n",
        "        D_max = max(min_dists)\n",
        "        if D_max > threshold_ratio * euclidean(centers[0], centers[1]):\n",
        "            new_center = X[np.argmax(min_dists)]\n",
        "            centers.append(new_center)\n",
        "        else:\n",
        "            break\n",
        "    return len(centers), np.array(centers)"
      ],
      "metadata": {
        "id": "lMGRsMK5dClp"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AKN: Calculate Initial Weights (Algorithm 2)\n",
        "def calculate_initial_weights(R, numcl):\n",
        "    R_min = R.min(axis=0)\n",
        "    R_max = R.max(axis=0)\n",
        "    N = (R - R_min) / (R_max - R_min + 1e-10)\n",
        "    C = N.mean(axis=0)\n",
        "    dmax = np.max([euclidean(n, C) for n in N])\n",
        "    X = np.zeros((numcl, R.shape[1]))\n",
        "    for j in range(numcl):\n",
        "        X[j, :] = C + np.random.uniform(-dmax, dmax, R.shape[1])\n",
        "    return X"
      ],
      "metadata": {
        "id": "2k2Is3sadFhB"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AKN: Clustering (Algorithm 3)\n",
        "def kohonen_clustering(X, initial_centers, num_epochs=100, initial_lr=0.5, initial_radius=0.5):\n",
        "    weights = initial_centers.copy()\n",
        "    num_neurons = len(weights)\n",
        "    sigma = initial_radius * num_neurons\n",
        "    sigma_decay = sigma / num_epochs\n",
        "    lr = initial_lr\n",
        "    for epoch in range(num_epochs):\n",
        "        lr = initial_lr * np.exp(-epoch / num_epochs)\n",
        "        sigma = max(0.1, sigma - sigma_decay * epoch)\n",
        "        for x in X:\n",
        "            distances = euclidean_distances([x], weights).flatten()\n",
        "            winner_idx = np.argmin(distances)\n",
        "            for j in range(num_neurons):\n",
        "                dist_to_winner = np.abs(j - winner_idx)\n",
        "                influence = np.exp(-dist_to_winner**2 / (2 * sigma**2 + 1e-10))\n",
        "                weights[j] += lr * influence * (x - weights[j])\n",
        "    assignments = [np.argmin(euclidean_distances([x], weights)) for x in X]\n",
        "    return assignments, weights"
      ],
      "metadata": {
        "id": "BdzbRuU1dIJz"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply AKN on original data\n",
        "X_clustering = X_scaled_data.drop(columns=['DU_ID', 'Label', 'Entropy_DU', 'Pearson_Corr']).values\n",
        "num_neurons, initial_centers = calculate_optimal_neurons(X_clustering, threshold_ratio=0.5)\n",
        "initial_weights = calculate_initial_weights(X_clustering, num_neurons)\n",
        "cluster_labels, final_weights = kohonen_clustering(X_clustering, initial_weights, num_epochs=100)\n",
        "X_scaled_data['Cluster'] = cluster_labels\n",
        "print(\"Clustering Silhouette Score:\", silhouette_score(X_clustering, cluster_labels) if len(set(cluster_labels)) > 1 else \"Single cluster detected\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFunwpLGdN5y",
        "outputId": "bb64979b-bd5c-4edd-a1aa-bd19e031ae8d"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clustering Silhouette Score: 0.9875121874592766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute MAD on original data\n",
        "mean_C = (X_scaled_data['TCP_Packets_mean'] / (X_scaled_data['TCP_Packets_mean'] + 1e-10)).mean()\n",
        "std_C = (X_scaled_data['TCP_Packets_mean'] / (X_scaled_data['TCP_Packets_mean'] + 1e-10)).std() or 1e-10\n",
        "mean_H = X_scaled_data['Entropy_DU'].mean()\n",
        "std_H = X_scaled_data['Entropy_DU'].std() or 1e-10\n",
        "mean_P = X_scaled_data['Pearson_Corr'].mean()\n",
        "std_P = X_scaled_data['Pearson_Corr'].std() or 1e-10\n",
        "\n",
        "mad_scores = []\n",
        "for _, row in X_scaled_data.iterrows():\n",
        "    C = (row['TCP_Packets_mean'] / (row['TCP_Packets_mean'] + 1e-10) - mean_C) / std_C\n",
        "    H = (row['Entropy_DU'] - mean_H) / std_H\n",
        "    P = (row['Pearson_Corr'] - mean_P) / std_P\n",
        "    mad = np.sqrt(0.4 * C**2 + 0.3 * H**2 + 0.3 * P**2)\n",
        "    mad_scores.append(mad if not np.isnan(mad) else 0)\n",
        "X_scaled_data['MAD'] = mad_scores\n",
        "X_scaled_data['MAD'] = scaler.fit_transform(X_scaled_data[['MAD']]).flatten()\n",
        "X_scaled_data['MAD'] = X_scaled_data['MAD'].fillna(0)"
      ],
      "metadata": {
        "id": "4Tf2ewYBdQ-z"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Balance data for threshold optimization\n",
        "# X_array = X_scaled_data.drop(columns=['DU_ID', 'Label', 'Entropy_DU', 'Pearson_Corr', 'Cluster', 'MAD']).values\n",
        "# y_array = X_scaled_data['Label'].values\n",
        "# try:\n",
        "#     smote = SMOTE(random_state=42, k_neighbors=min(5, len(y_array[y_array == 1]) - 1))\n",
        "#     X_resampled, y_resampled = smote.fit_resample(X_array, y_array)\n",
        "# except ValueError:\n",
        "#     X_resampled, y_resampled = X_array, y_array\n",
        "# X_scaled_data_resampled = pd.DataFrame(X_resampled, columns=X_scaled_data.drop(columns=['DU_ID', 'Label', 'Entropy_DU', 'Pearson_Corr', 'Cluster', 'MAD']).columns)\n",
        "# X_scaled_data_resampled['Label'] = y_resampled\n",
        "# X_scaled_data_resampled['MAD'] = scaler.transform(X_scaled_data[['MAD']].iloc[:len(X_resampled)].values).flatten()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kNIRu1yNdVfF"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Balance data for threshold optimization\n",
        "X_array = X_scaled_data.drop(columns=['DU_ID', 'Label', 'Entropy_DU', 'Pearson_Corr', 'Cluster', 'MAD']).values\n",
        "y_array = X_scaled_data['Label'].values\n",
        "try:\n",
        "    smote = SMOTE(random_state=42, k_neighbors=min(5, max(1, len(y_array[y_array == 1]) - 1)))\n",
        "    X_resampled, y_resampled = smote.fit_resample(X_array, y_array)\n",
        "except ValueError:\n",
        "    X_resampled, y_resampled = X_array, y_array\n",
        "X_scaled_data_resampled = pd.DataFrame(X_resampled, columns=X_scaled_data.drop(columns=['DU_ID', 'Label', 'Entropy_DU', 'Pearson_Corr', 'Cluster', 'MAD']).columns)\n",
        "X_scaled_data_resampled['Label'] = y_resampled\n",
        "# Ensure MAD values align with resampled data length\n",
        "mad_values = X_scaled_data['MAD'].values\n",
        "if len(X_resampled) > len(mad_values):\n",
        "    # Pad with mean MAD if resampled data is longer\n",
        "    mad_extended = np.pad(mad_values, (0, len(X_resampled) - len(mad_values)), mode='constant', constant_values=mad_values.mean())\n",
        "else:\n",
        "    mad_extended = mad_values[:len(X_resampled)]\n",
        "X_scaled_data_resampled['MAD'] = scaler.transform(mad_extended.reshape(-1, 1)).flatten()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNco9qtpe2ga",
        "outputId": "60ab6b80-ebe4-4cdf-8622-84e97bdb4a8e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RobustScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimize Thresholds\n",
        "fpr, tpr, thresholds = roc_curve(X_scaled_data_resampled['Label'], X_scaled_data_resampled['MAD'])\n",
        "optimal_idx = np.argmax(tpr - fpr)\n",
        "optimal_z = (thresholds[optimal_idx] - X_scaled_data_resampled['MAD'].mean()) / (X_scaled_data_resampled['MAD'].std() + 1e-10)\n",
        "LMAD = X_scaled_data_resampled['MAD'].mean() + optimal_z * X_scaled_data_resampled['MAD'].std()\n",
        "X_scaled_data['Abnormal_DU'] = (X_scaled_data['MAD'] > LMAD).astype(int)\n",
        "\n",
        "best_ladur = 0.55\n",
        "best_f1 = 0\n",
        "for ladur in [0.5, 0.55, 0.6]:\n",
        "    cluster_votes = X_scaled_data.groupby('Cluster')['Abnormal_DU'].mean()\n",
        "    abnormal_clusters = cluster_votes[cluster_votes >= ladur].index\n",
        "    X_scaled_data['Predicted_Label'] = X_scaled_data['Cluster'].apply(lambda c: 1 if c in abnormal_clusters else 0)\n",
        "    f1 = f1_score(X_scaled_data['Label'], X_scaled_data['Predicted_Label'])\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_ladur = ladur"
      ],
      "metadata": {
        "id": "dkCRko53dbnk"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Detection\n",
        "LMAD = X_scaled_data['MAD'].mean() + optimal_z * X_scaled_data['MAD'].std()\n",
        "X_scaled_data['Abnormal_DU'] = (X_scaled_data['MAD'] > LMAD).astype(int)\n",
        "cluster_votes = X_scaled_data.groupby('Cluster')['Abnormal_DU'].mean()\n",
        "abnormal_clusters = cluster_votes[cluster_votes >= best_ladur].index\n",
        "X_scaled_data['Predicted_Label'] = X_scaled_data['Cluster'].apply(lambda c: 1 if c in abnormal_clusters else 0)\n",
        "X_scaled_data['Predicted_Label'] = X_scaled_data['Predicted_Label'].fillna(0)\n",
        "print(\"Final Detection Evaluation:\\n\")\n",
        "print(classification_report(X_scaled_data['Label'], X_scaled_data['Predicted_Label']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fw4xDL2Rdhhl",
        "outputId": "24ee9d5e-43b6-43d8-c2bb-37583b148372"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Detection Evaluation:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.05      1.00      0.10        38\n",
            "           1       0.00      0.00      0.00       695\n",
            "\n",
            "    accuracy                           0.05       733\n",
            "   macro avg       0.03      0.50      0.05       733\n",
            "weighted avg       0.00      0.05      0.01       733\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validate Random Forest\n",
        "# rf = RandomForestClassifier(random_state=42)\n",
        "# scores = cross_val_score(rf, X_array, X_scaled_data['Label'], cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), scoring='f1_weighted')\n",
        "# print(\"Random Forest Cross-Validation F1 Scores:\", scores, \"Mean:\", scores.mean())\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "X_array = X_scaled_data.drop(columns=['DU_ID', 'Label', 'Entropy_DU', 'Pearson_Corr', 'Cluster', 'MAD']).values\n",
        "y_array = X_scaled_data['Label'].values\n",
        "rf.fit(X_array, y_array)\n",
        "y_pred_rf = rf.predict(X_array)\n",
        "print(\"Random Forest Classification Report:\\n\")\n",
        "print(classification_report(y_array, y_pred_rf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WyS4l5jdkHR",
        "outputId": "14c96081-d3ab-45e3-b978-d25c6110dfa2"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        38\n",
            "           1       1.00      1.00      1.00       695\n",
            "\n",
            "    accuracy                           1.00       733\n",
            "   macro avg       1.00      1.00      1.00       733\n",
            "weighted avg       1.00      1.00      1.00       733\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "# # Feature Selection using Random Forest\n",
        "# rf = RandomForestClassifier(random_state=42)\n",
        "# X_array = X_scaled_data.drop(columns=['DU_ID', 'Label', 'Entropy_DU', 'Pearson_Corr', 'Cluster', 'MAD']).values\n",
        "# y_array = X_scaled_data['Label'].values\n",
        "# rf.fit(X_array, y_array)\n",
        "# feature_importances = pd.Series(rf.feature_importances_, index=X_scaled_data.drop(columns=['DU_ID', 'Label', 'Entropy_DU', 'Pearson_Corr', 'Cluster', 'MAD']).columns)\n",
        "# top_features = feature_importances.nlargest(20).index\n",
        "# X_array = X_scaled_data[top_features].values\n",
        "# print(\"Top 20 Features:\\n\", top_features)\n",
        "\n",
        "# # Split data for proper evaluation\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_array, y_array, test_size=0.2, stratify=y_array, random_state=42)\n",
        "\n",
        "# # Balance training data\n",
        "# try:\n",
        "#     smote = SMOTE(random_state=42, k_neighbors=min(5, max(1, len(y_train[y_train == 1]) - 1)))\n",
        "#     X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "# except ValueError:\n",
        "#     X_train_resampled, y_train_resampled = X_train, y_train\n",
        "\n",
        "# # Optimize Thresholds for AKN-FGD\n",
        "# X_scaled_data_resampled = pd.DataFrame(X_train_resampled, columns=top_features)\n",
        "# X_scaled_data_resampled['Label'] = y_train_resampled\n",
        "# mad_values = X_scaled_data['MAD'].values\n",
        "# if len(X_train_resampled) > len(mad_values):\n",
        "#     mad_extended = np.pad(mad_values, (0, len(X_train_resampled) - len(mad_values)), mode='constant', constant_values=mad_values.mean())\n",
        "# else:\n",
        "#     mad_extended = mad_values[:len(X_train_resampled)]\n",
        "# X_scaled_data_resampled['MAD'] = scaler.transform(mad_extended.reshape(-1, 1)).flatten()\n",
        "# # Extend or truncate Cluster values to match resampled data length\n",
        "# cluster_values = X_scaled_data['Cluster'].values\n",
        "# if len(X_train_resampled) > len(cluster_values):\n",
        "#     cluster_extended = np.pad(cluster_values, (0, len(X_train_resampled) - len(cluster_values)), mode='constant', constant_values=np.median(cluster_values))\n",
        "# else:\n",
        "#     cluster_extended = cluster_values[:len(X_train_resampled)]\n",
        "# X_scaled_data_resampled['Cluster'] = cluster_extended\n",
        "\n",
        "# fpr, tpr, thresholds = roc_curve(X_scaled_data_resampled['Label'], X_scaled_data_resampled['MAD'])\n",
        "# optimal_idx = np.argmax(tpr - fpr)\n",
        "# optimal_z = (thresholds[optimal_idx] - X_scaled_data_resampled['MAD'].mean()) / (X_scaled_data_resampled['MAD'].std() + 1e-10)\n",
        "# LMAD = X_scaled_data_resampled['MAD'].mean() + optimal_z * X_scaled_data_resampled['MAD'].std()\n",
        "# X_scaled_data['Abnormal_DU'] = (X_scaled_data['MAD'] > LMAD).astype(int)\n",
        "\n",
        "# best_ladur = 0.55\n",
        "# best_f1 = 0\n",
        "# for ladur in [0.3, 0.4, 0.5, 0.55, 0.6, 0.7]:\n",
        "#     cluster_votes = X_scaled_data.groupby('Cluster')['Abnormal_DU'].mean()\n",
        "#     abnormal_clusters = cluster_votes[cluster_votes >= ladur].index\n",
        "#     X_scaled_data['Predicted_Label'] = X_scaled_data['Cluster'].apply(lambda c: 1 if c in abnormal_clusters else 0)\n",
        "#     f1 = f1_score(X_scaled_data['Label'], X_scaled_data['Predicted_Label'])\n",
        "#     if f1 > best_f1:\n",
        "#         best_f1 = f1\n",
        "#         best_ladur = ladur\n",
        "\n",
        "# # Final Detection for AKN-FGD\n",
        "# LMAD = X_scaled_data['MAD'].mean() + optimal_z * X_scaled_data['MAD'].std()\n",
        "# X_scaled_data['Abnormal_DU'] = (X_scaled_data['MAD'] > LMAD).astype(int)\n",
        "# cluster_votes = X_scaled_data.groupby('Cluster')['Abnormal_DU'].mean()\n",
        "# abnormal_clusters = cluster_votes[cluster_votes >= best_ladur].index\n",
        "# X_scaled_data['Predicted_Label'] = X_scaled_data['Cluster'].apply(lambda c: 1 if c in abnormal_clusters else 0)\n",
        "# X_scaled_data['Predicted_Label'] = X_scaled_data['Predicted_Label'].fillna(0)\n",
        "# print(\"Final Detection Evaluation (AKN-FGD):\\n\")\n",
        "# print(classification_report(X_scaled_data['Label'], X_scaled_data['Predicted_Label']))\n",
        "\n",
        "# # Random Forest Evaluation with Cross-Validation\n",
        "# rf = RandomForestClassifier(max_depth=10, min_samples_split=5, random_state=42)\n",
        "# scores = cross_val_score(rf, X_train, y_train, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), scoring='f1_weighted')\n",
        "# print(\"Random Forest Cross-Validation F1 Scores:\", scores, \"Mean:\", scores.mean())\n",
        "# rf.fit(X_train, y_train)\n",
        "# y_pred_rf = rf.predict(X_test)\n",
        "# print(\"Random Forest Test Set Classification Report:\\n\")\n",
        "# print(classification_report(y_test, y_pred_rf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUoyVLUMi2hK",
        "outputId": "f451d42c-fdd5-40dd-81fd-ef7ed8469488"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 20 Features:\n",
            " Index(['Connection_Duration_mean', 'Length_max', 'Backward_Packets_mean',\n",
            "       'Length_mean', 'Packet_Rate_mean', 'Flow_Packets_Per_Second_mean',\n",
            "       'Flow_Bytes_Per_Second_mean', 'Inter_Arrival_Time_std',\n",
            "       'Ratio_Fwd_Bwd_max', 'Ratio_Fwd_Bwd_mean', 'Forward_Packets_max',\n",
            "       'Connection_Duration_min', 'Flow_Bytes_Per_Second_max',\n",
            "       'Backward_Packets_min', 'Inter_Arrival_Time_mean', 'Packet_Rate_max',\n",
            "       'Flow_Packets_Per_Second_max', 'Flow_Packets_Per_Second_std',\n",
            "       'Ratio_Fwd_Bwd_std', 'Packet_Rate_std'],\n",
            "      dtype='object')\n",
            "Final Detection Evaluation (AKN-FGD):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        38\n",
            "           1       0.03      0.00      0.00       695\n",
            "\n",
            "    accuracy                           0.00       733\n",
            "   macro avg       0.01      0.00      0.00       733\n",
            "weighted avg       0.02      0.00      0.00       733\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RobustScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Cross-Validation F1 Scores: [0.99183237 1.         1.         1.         1.        ] Mean: 0.998366473535585\n",
            "Random Forest Test Set Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         8\n",
            "           1       1.00      1.00      1.00       139\n",
            "\n",
            "    accuracy                           1.00       147\n",
            "   macro avg       1.00      1.00      1.00       147\n",
            "weighted avg       1.00      1.00      1.00       147\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Lightweight Neural Network\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense, Dropout\n",
        "# from tensorflow.keras.callbacks import EarlyStopping\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# model = Sequential([\n",
        "#     Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "#     Dropout(0.2),\n",
        "#     Dense(16, activation='relu'),\n",
        "#     Dropout(0.2),\n",
        "#     Dense(1, activation='sigmoid')\n",
        "# ])\n",
        "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "# history = model.fit(X_train_resampled, y_train_resampled, validation_split=0.2, epochs=50, batch_size=32, callbacks=[early_stopping], verbose=0)\n",
        "# y_pred_nn = (model.predict(X_test) > 0.5).astype(int)\n",
        "# print(\"Neural Network Test Set Classification Report:\\n\")\n",
        "# print(classification_report(y_test, y_pred_nn))"
      ],
      "metadata": {
        "id": "AlcVoLh6kLXQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}