{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "13RS7eunwA45WyeiMVLJ-NeGeAILlA33e",
      "authorship_tag": "ABX9TyPfkgzcbG+OnoJNgjn+o6Db",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Munazza-Farees/NITW-SIP2025-Project/blob/main/AKN_FGD_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI7UUv2IONLD"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pywt\n",
        "from sklearn.metrics import classification_report, roc_curve, f1_score, silhouette_score\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from scipy.spatial.distance import euclidean\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import pearsonr\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "URL = '/content/drive/MyDrive/Colab Notebooks/Client1 (copy).csv'\n",
        "data = pd.read_csv(URL)"
      ],
      "metadata": {
        "id": "EBUj6UB3UeFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input missing values\n",
        "data.fillna(data.median(numeric_only=True), inplace=True)\n",
        "data['Protocol'].fillna(data['Protocol'].mode()[0], inplace=True)\n",
        "data['Flags'].fillna(data['Flags'].mode()[0], inplace=True)\n",
        "\n",
        "# Convert types and encode categorical features\n",
        "data = data.astype({'Time':'float', 'Label': int})\n",
        "data = pd.get_dummies(data, columns=['Protocol', 'Flags'], prefix=['Protocol', 'Flags'])\n",
        "data = data.drop(columns=['Source', 'Destination'], errors='ignore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4Hryfj5DUnfi",
        "outputId": "e662edf0-3e44-4c5d-b36c-f24cdfe60d72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-78-890462510>:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['Protocol'].fillna(data['Protocol'].mode()[0], inplace=True)\n",
            "<ipython-input-78-890462510>:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['Flags'].fillna(data['Flags'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequence Alignment with Smith-Waterman Algorithm\n",
        "def smith_waterman(seq1, seq2, match_score=2, mismatch_score=-2, gap_penalty=-1):\n",
        "    n, m = len(seq1), len(seq2)\n",
        "    H = np.zeros((n+1, m+1))\n",
        "    for i in range(1, n+1):\n",
        "        for j in range(1, m+1):\n",
        "            match = H[i-1, j-1] + (match_score if seq1[i-1] == seq2[j-1] else mismatch_score)\n",
        "            delete = H[i-1, j] + gap_penalty\n",
        "            insert = H[i, j-1] + gap_penalty\n",
        "            H[i, j] = max(match, delete, insert, 0)\n",
        "    return np.max(H)\n",
        "\n",
        "# Construct benchmark sequences for RTO values\n",
        "rto_values = [1, 2]\n",
        "benchmark_sequences = {rto: np.array([1 if i % rto < 0.1 else 0 for i in np.arange(0, 60, 0.1)]) for rto in rto_values}"
      ],
      "metadata": {
        "id": "nKHCEXX8V2rF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimate DU duration using UDP packet counts\n",
        "cw_duration = 60\n",
        "data['CW_ID'] = ((data['Time'] - data['Time'].min()) // cw_duration).astype(int)\n",
        "udp_data = data[data['Protocol_UDP'] == 1]  # Assuming one-hot encoded 'Protocol_UDP'\n",
        "cw_udp_counts = udp_data.groupby('CW_ID')['Length'].count().reindex(data['CW_ID'].unique(), fill_value=0)\n",
        "cw_udp_seq = cw_udp_counts.values\n",
        "\n",
        "max_score = -np.inf\n",
        "time_du = 1\n",
        "for rto, seq in benchmark_sequences.items():\n",
        "    score = smith_waterman(cw_udp_seq[:len(seq)], seq)\n",
        "    if score > max_score:\n",
        "        max_score = score\n",
        "        time_du = rto\n",
        "\n",
        "# Create DUs based on estimated TimeDU\n",
        "data['DU_ID'] = ((data['Time'] - data['Time'].min()) // time_du).astype(int)\n",
        "print(\"Estimated DU Duration:\", time_du, \"seconds\")\n",
        "print(\"Packets per DU:\\n\", data.groupby('DU_ID').size().describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJXHrloDXg49",
        "outputId": "3c953ab9-34b0-4e4a-87d3-9495fec01c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated DU Duration: 2 seconds\n",
            "Packets per DU:\n",
            " count    733.000000\n",
            "mean      47.553888\n",
            "std      131.489508\n",
            "min        2.000000\n",
            "25%        8.000000\n",
            "50%       14.000000\n",
            "75%       24.000000\n",
            "max      733.000000\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Aggregation\n",
        "data['TCP_Packets'] = data['Protocol_TCP']  # Assuming one-hot encoded 'Protocol_TCP'\n",
        "data['Total_Packets'] = 1   # Each row is a packet\n",
        "data['Burstiness'] = data.groupby('DU_ID')['Packet_Rate'].transform(lambda x:x.max() / (x.mean() + 1e-10))\n",
        "\n",
        "features_to_aggregate = [\n",
        "    'Length', 'Inter_Arrival_Time', 'Connection_Duration', 'Packet_Rate',\n",
        "    'Flow_Bytes_Per_Second', 'Flow_Packets_Per_Second', 'Forward_Packets',\n",
        "    'Backward_Packets', 'Ratio_Fwd_Bwd', 'Entropy', 'Packet_Size_Variance',\n",
        "    'Burstiness', 'TCP_Packets', 'Total_Packets'\n",
        "]\n",
        "\n",
        "agg_funcs = {col: ['mean', 'std', 'max', 'min'] for col in features_to_aggregate}\n",
        "agg_funcs['Label'] = lambda x: 1 if (x == 1).mean() >= 0.05 else 0\n",
        "agg_data = data.groupby('DU_ID').agg(agg_funcs)\n",
        "agg_data.columns = ['_'.join(col) if isinstance(col, tuple) else col for col in agg_data.columns]\n",
        "agg_data.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "AilrpVYJX3Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute wavelet packet entropy per DU\n",
        "def compute_du_entropy(group):\n",
        "    signal = group['Length'].values\n",
        "    if len(signal) < 2:\n",
        "        return 0\n",
        "    wp = pywt.WaveletPacket(data=signal, wavelet='db1', mode='symmetric', maxlevel=3)\n",
        "    energies = [np.sum(np.square(node.data)) for node in wp.get_level(3)]\n",
        "    total_energies = np.sum(energies)\n",
        "    probs = np.array(energies) / (total_energies + 1e-10)\n",
        "    return -np.sum(probs * np.log2(probs + 1e-10))\n",
        "\n",
        "agg_data['Entropy'] = data.groupby('DU_ID').apply(compute_du_entropy).reindex(agg_data.index).values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx24UxDiax9O",
        "outputId": "0f93ef98-9dd9-411d-9342-1ce138c5021b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-82-2986227820>:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  agg_data['Entropy'] = data.groupby('DU_ID').apply(compute_du_entropy).reindex(agg_data.index).values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature scaling\n",
        "\n",
        "agg_data_features = agg_data.drop(columns=['DU_ID', 'Label_<lambda>'])\n",
        "\n",
        "agg_data_features.fillna(0, inplace=True)\n",
        "\n",
        "for col in agg_data_features.columns:\n",
        "    if agg_data_features[col].dtype == 'bool':\n",
        "        agg_data_features[col] = agg_data_features[col].astype(int)\n",
        "\n",
        "\n",
        "for col in agg_data_features.columns:\n",
        "    p1, p99 = agg_data_features[col].quantile([0.01, 0.99])\n",
        "    agg_data_features[col] = agg_data_features[col].clip(p1, p99)\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(agg_data_features)\n",
        "X_scaled_data = pd.DataFrame(X_scaled, columns=agg_data_features.columns)\n",
        "\n",
        "\n",
        "X_scaled_data['DU_ID'] = agg_data['DU_ID']\n",
        "# X_scaled_data['Entropy_DU'] = X_scaled_data['DU_ID'].map(agg_data['Entropy'])\n",
        "X_scaled_data['Label'] = agg_data['Label_<lambda>']\n",
        "X_scaled_data.fillna(0, inplace=True)\n",
        "\n",
        "# Balance DUs with SMOTE\n",
        "X_array = X_scaled_data.drop(columns=['DU_ID', 'Label']).values\n",
        "y_array = X_scaled_data['Label'].values\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "X_resampled, y_resampled = smote.fit_resample(X_array, y_array)\n",
        "X_scaled_data_resampled = pd.DataFrame(X_resampled, columns=X_scaled_data.drop(columns=['DU_ID', 'Label']).columns)\n",
        "\n",
        "X_scaled_data_resampled['Label'] = y_resampled\n",
        "# X_scaled_data_resampled['DU_ID'] = X_scaled_data['DU_ID'].iloc[:len(X_resampled)].values\n",
        "# X_scaled_data_resampled['Entropy_DU'] = X_scaled_data_resampled['DU_ID'].map(agg_data['Entropy'])"
      ],
      "metadata": {
        "id": "VdlDVwlUbt7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AKN: Calculate Neurons' Number (Algorithm 1)\n",
        "def calculate_optimal_neurons(X, threshold_ratio=0.5):\n",
        "    centers = [X[0]]\n",
        "    max_dist = 0\n",
        "    second = X[1]\n",
        "    for row in X:\n",
        "        dist = euclidean(row, centers[0])\n",
        "        if dist > max_dist:\n",
        "            max_dist = dist\n",
        "            second = row\n",
        "    centers.append(second)\n",
        "    while True:\n",
        "        min_dists = [min([euclidean(row, center) for center in centers]) for row in X]\n",
        "        D_max = max(min_dists)\n",
        "        if D_max > threshold_ratio * euclidean(centers[0], centers[1]):\n",
        "            new_center = X[np.argmax(min_dists)]\n",
        "            centers.append(new_center)\n",
        "        else:\n",
        "            break\n",
        "    return len(centers), np.array(centers)\n",
        "\n",
        "# AKN: Calculate Initial Weights (Algorithm 2)\n",
        "def calculate_initial_weights(R, numcl):\n",
        "    N = (R - R.min(axis=0)) / (R.max(axis=0) - R.min(axis=0) + 1e-10)\n",
        "    C = N.mean(axis=0)\n",
        "    dmax = np.max([euclidean(n, C) for n in N])\n",
        "    X = np.zeros((R.shape[1], numcl))\n",
        "    for j in range(numcl):\n",
        "        X[:, j] = C + np.random.uniform(-dmax, dmax, R.shape[1])\n",
        "    return X.T\n",
        "\n",
        "# AKN: Clustering (Algorithm 3)\n",
        "def kohonen_clustering(X, initial_centers, num_epochs=100, initial_lr=0.5, initial_radius=0.5):\n",
        "    weights = initial_centers.copy()\n",
        "    num_neurons = len(weights)\n",
        "    sigma = initial_radius * num_neurons\n",
        "    sigma_decay = sigma / num_epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        lr = initial_lr * np.exp(-epoch / num_epochs)\n",
        "        sigma = sigma - sigma_decay * epoch\n",
        "        for x in X:\n",
        "            distances = euclidean_distances([x], weights).flatten()\n",
        "            winner_idx = np.argmin(distances)\n",
        "            for j in range(num_neurons):\n",
        "                dist_to_winner = np.abs(j - winner_idx)\n",
        "                influence = np.exp(-dist_to_winner**2 / (2 * sigma**2))\n",
        "                weights[j] += lr * influence * (x - weights[j])\n",
        "    assignments = [np.argmin(euclidean_distances([x], weights)) for x in X]\n",
        "    return assignments, weights"
      ],
      "metadata": {
        "id": "cWl-gfk3doOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply AKN\n",
        "num_neurons, initial_centers = calculate_optimal_neurons(X_resampled, threshold_ratio=0.5)\n",
        "initial_weights = calculate_initial_weights(X_resampled, num_neurons)\n",
        "cluster_labels, final_weights = kohonen_clustering(X_resampled, initial_weights, num_epochs=100)\n",
        "X_scaled_data_resampled['Cluster'] = cluster_labels\n",
        "print(\"Clustering Silhouette Score:\", silhouette_score(X_resampled, cluster_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBx4hs6ue96n",
        "outputId": "491e444a-7c3b-4cf9-9400-5eb4848e6653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clustering Silhouette Score: 0.7575332247943308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_scaled_data_resampled['DU_ID'] = X_scaled_data['DU_ID'].iloc[:len(X_scaled_data_resampled)].values\n",
        "X_scaled_data_resampled['Entropy_DU'] = X_scaled_data_resampled['DU_ID'].map(agg_data['Entropy'])\n",
        "\n",
        "# Compute MAD\n",
        "mean_C = (X_scaled_data_resampled['TCP_Packets_mean'] / (X_scaled_data_resampled['TCP_Packets_mean'] + 1e-10)).mean()\n",
        "std_C = (X_scaled_data_resampled['TCP_Packets_mean'] / (X_scaled_data_resampled['TCP_Packets_mean'] + 1e-10)).std()\n",
        "mean_H = X_scaled_data_resampled['Entropy_DU'].mean()\n",
        "std_H = X_scaled_data_resampled['Entropy_DU'].std()\n",
        "mean_P = X_scaled_data_resampled.apply(lambda row: pearsonr(row['TCP_Packets_mean'], row['Total_Packets_mean'])[0] if row['Total_Packets_mean'] > 0 else 0, axis=1).mean()\n",
        "std_P = X_scaled_data_resampled.apply(lambda row: pearsonr(row['TCP_Packets_mean'], row['Total_Packets_mean'])[0] if row['Total_Packets_mean'] > 0 else 0, axis=1).std()\n",
        "\n",
        "mad_scores = []\n",
        "for _, row in X_scaled_data_resampled.iterrows():\n",
        "    C = (row['TCP_Packets_mean'] / (row['TCP_Packets_mean'] + 1e-10) - mean_C) / (std_C + 1e-10)\n",
        "    H = (row['Entropy_DU'] - mean_H) / (std_H + 1e-10)\n",
        "    P = (pearsonr([row['TCP_Packets_mean']], [row['Total_Packets_mean']])[0] - mean_P) / (std_P + 1e-10) if row['Total_Packets_mean'] > 0 else 0\n",
        "    mad = np.sqrt(0.4 * C**2 + 0.3 * H**2 + 0.3 * P**2)\n",
        "    mad_scores.append(mad)\n",
        "X_scaled_data_resampled['MAD'] = mad_scores\n",
        "X_scaled_data_resampled['MAD'] = scaler.fit_transform(X_scaled_data_resampled[['MAD']]).flatten()\n",
        "\n",
        "# Optimize Thresholds\n",
        "fpr, tpr, thresholds = roc_curve(X_scaled_data_resampled['Label'], X_scaled_data_resampled['MAD'])\n",
        "optimal_idx = np.argmax(tpr - fpr)\n",
        "optimal_z = (thresholds[optimal_idx] - X_scaled_data_resampled['MAD'].mean()) / X_scaled_data_resampled['MAD'].std()\n",
        "LMAD = X_scaled_data_resampled['MAD'].mean() + optimal_z * X_scaled_data_resampled['MAD'].std()\n",
        "X_scaled_data_resampled['Abnormal_DU'] = (X_scaled_data_resampled['MAD'] > LMAD).astype(int)\n",
        "\n",
        "best_ladur = 0.55\n",
        "best_f1 = 0\n",
        "for ladur in [0.5, 0.55, 0.6]:\n",
        "    cluster_votes = X_scaled_data_resampled.groupby('Cluster')['Abnormal_DU'].mean()\n",
        "    abnormal_clusters = cluster_votes[cluster_votes >= ladur].index\n",
        "    X_scaled_data_resampled['Predicted_Label'] = X_scaled_data_resampled['Cluster'].apply(lambda c: 1 if c in abnormal_clusters else 0)\n",
        "    f1 = f1_score(X_scaled_data_resampled['Label'], X_scaled_data_resampled['Predicted_Label'])\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_ladur = ladur\n",
        "\n",
        "# Final Detection\n",
        "LMAD = X_scaled_data_resampled['MAD'].mean() + optimal_z * X_scaled_data_resampled['MAD'].std()\n",
        "X_scaled_data_resampled['Abnormal_DU'] = (X_scaled_data_resampled['MAD'] > LMAD).astype(int)\n",
        "cluster_votes = X_scaled_data_resampled.groupby('Cluster')['Abnormal_DU'].mean()\n",
        "abnormal_clusters = cluster_votes[cluster_votes >= best_ladur].index\n",
        "X_scaled_data_resampled['Predicted_Label'] = X_scaled_data_resampled['Cluster'].apply(lambda c: 1 if c in abnormal_clusters else 0)\n",
        "X_scaled_data['Predicted_Label'] = X_scaled_data['DU_ID'].map(X_scaled_data_resampled.groupby('DU_ID')['Predicted_Label'].first())\n",
        "print(\"Final Detection Evaluation:\\n\")\n",
        "print(classification_report(X_scaled_data['Label'], X_scaled_data['Predicted_Label']))\n",
        "\n",
        "# Cross-validate Random Forest\n",
        "from sklearn.model_selection import cross_val_score\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "scores = cross_val_score(rf, X_array, X_scaled_data['Label'], cv=5, scoring='f1_weighted')\n",
        "print(\"Random Forest Cross-Validation F1 Scores:\", scores, \"Mean:\", scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "e_W3QRgEgewE",
        "outputId": "e0a8d3ad-7ebc-4184-d1a6-41c3cb5cb916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Length of values (733) does not match length of index (1390)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-2186146825>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_scaled_data_resampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DU_ID'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_scaled_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DU_ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_scaled_data_resampled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_scaled_data_resampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Entropy_DU'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_scaled_data_resampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DU_ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Entropy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Compute MAD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmean_C\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_scaled_data_resampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TCP_Packets_mean'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_scaled_data_resampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TCP_Packets_mean'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4309\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4310\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4311\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4522\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4523\u001b[0m         \"\"\"\n\u001b[0;32m-> 4524\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4526\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5266\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5267\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5268\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \"\"\"\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    574\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length of values (733) does not match length of index (1390)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pywt\n",
        "from sklearn.metrics import classification_report, roc_curve, f1_score, silhouette_score\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from scipy.spatial.distance import euclidean\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import pearsonr\n",
        "from collections import Counter\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Load dataset\n",
        "URL = '/content/drive/MyDrive/Colab Notebooks/Client1 (copy).csv'\n",
        "data = pd.read_csv(URL)\n",
        "\n",
        "# Impute missing values\n",
        "data.fillna(data.median(numeric_only=True), inplace=True)\n",
        "data['Protocol'].fillna(data['Protocol'].mode()[0], inplace=True)\n",
        "data['Flags'].fillna(data['Flags'].mode()[0], inplace=True)\n",
        "\n",
        "# Convert types and encode categorical features\n",
        "data = data.astype({'Time': 'float', 'Label': int})\n",
        "data = pd.get_dummies(data, columns=['Protocol', 'Flags'], prefix=['Protocol', 'Flags'])\n",
        "data = data.drop(columns=['Source', 'Destination'], errors='ignore')\n",
        "\n",
        "# Sequence Alignment with Smith-Waterman Algorithm\n",
        "def smith_waterman(seq1, seq2, match_score=2, mismatch_score=-2, gap_penalty=-1):\n",
        "    n, m = len(seq1), len(seq2)\n",
        "    H = np.zeros((n+1, m+1))\n",
        "    for i in range(1, n+1):\n",
        "        for j in range(1, m+1):\n",
        "            match = H[i-1, j-1] + (match_score if seq1[i-1] == seq2[j-1] else mismatch_score)\n",
        "            delete = H[i-1, j] + gap_penalty\n",
        "            insert = H[i, j-1] + gap_penalty\n",
        "            H[i, j] = max(match, delete, insert, 0)\n",
        "    return np.max(H)\n",
        "\n",
        "# Construct benchmark sequences for RTO values\n",
        "rto_values = [1, 2]\n",
        "benchmark_sequences = {rto: np.array([1 if i % rto < 0.1 else 0 for i in np.arange(0, 60, 0.1)]) for rto in rto_values}\n",
        "\n",
        "# Estimate DU duration using UDP packet counts\n",
        "cw_duration = 60\n",
        "data['CW_ID'] = ((data['Time'] - data['Time'].min()) // cw_duration).astype(int)\n",
        "udp_data = data[data['Protocol_UDP'] == 1]  # Assuming one-hot encoded 'Protocol_UDP'\n",
        "cw_udp_counts = udp_data.groupby('CW_ID')['Length'].count().reindex(data['CW_ID'].unique(), fill_value=0)\n",
        "cw_udp_seq = cw_udp_counts.values\n",
        "\n",
        "max_score = -np.inf\n",
        "time_du = 1\n",
        "for rto, seq in benchmark_sequences.items():\n",
        "    score = smith_waterman(cw_udp_seq[:len(seq)], seq)\n",
        "    if score > max_score:\n",
        "        max_score = score\n",
        "        time_du = rto\n",
        "\n",
        "# Create DUs based on estimated TimeDU\n",
        "data['DU_ID'] = ((data['Time'] - data['Time'].min()) // time_du).astype(int)\n",
        "print(\"Estimated DU Duration:\", time_du, \"seconds\")\n",
        "print(\"Packets per DU:\\n\", data.groupby('DU_ID').size().describe())\n",
        "\n",
        "# Feature Aggregation\n",
        "data['TCP_Packets'] = data['Protocol_TCP']\n",
        "data['Total_Packets'] = 1  # Each row is a packet\n",
        "data['Burstiness'] = data.groupby('DU_ID')['Packet_Rate'].transform(lambda x: x.max() / (x.mean() + 1e-10))\n",
        "\n",
        "features_to_aggregate = [\n",
        "    'Length', 'Inter_Arrival_Time', 'Connection_Duration', 'Packet_Rate',\n",
        "    'Flow_Bytes_Per_Second', 'Flow_Packets_Per_Second', 'Forward_Packets',\n",
        "    'Backward_Packets', 'Ratio_Fwd_Bwd', 'Entropy', 'Packet_Size_Variance',\n",
        "    'Burstiness', 'TCP_Packets', 'Total_Packets'\n",
        "]\n",
        "\n",
        "agg_funcs = {col: ['mean', 'std', 'max', 'min'] for col in features_to_aggregate}\n",
        "agg_funcs['Label'] = lambda x: 1 if (x == 1).mean() >= 0.05 else 0\n",
        "agg_data = data.groupby('DU_ID').agg(agg_funcs)\n",
        "agg_data.columns = ['_'.join(col) if isinstance(col, tuple) else col for col in agg_data.columns]\n",
        "agg_data.reset_index(inplace=True)\n",
        "\n",
        "# Compute wavelet packet entropy per DU\n",
        "def compute_du_entropy(group):\n",
        "    signal = group['Length'].values\n",
        "    if len(signal) < 2:\n",
        "        return 0\n",
        "    wp = pywt.WaveletPacket(data=signal, wavelet='db1', mode='symmetric', maxlevel=3)\n",
        "    energies = [np.sum(np.square(node.data)) for node in wp.get_level(3)]\n",
        "    total_energy = np.sum(energies)\n",
        "    probs = np.array(energies) / (total_energy + 1e-10)\n",
        "    return -np.sum(probs * np.log2(probs + 1e-10))\n",
        "\n",
        "agg_data['Entropy_DU'] = data.groupby('DU_ID').apply(compute_du_entropy).reindex(agg_data.index).values\n",
        "\n",
        "# Feature Scaling\n",
        "agg_data_features = agg_data.drop(columns=['DU_ID', 'Label_<lambda>'])\n",
        "agg_data_features.fillna(0, inplace=True)\n",
        "\n",
        "for col in agg_data_features.columns:\n",
        "    if agg_data_features[col].dtype == 'bool':\n",
        "        agg_data_features[col] = agg_data_features[col].astype(int)\n",
        "\n",
        "for col in agg_data_features.columns:\n",
        "    p1, p99 = agg_data_features[col].quantile([0.01, 0.99])\n",
        "    agg_data_features[col] = agg_data_features[col].clip(p1, p99)\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(agg_data_features)\n",
        "X_scaled_data = pd.DataFrame(X_scaled, columns=agg_data_features.columns)\n",
        "X_scaled_data['DU_ID'] = agg_data['DU_ID'].values\n",
        "X_scaled_data['Label'] = agg_data['Label_<lambda>'].values\n",
        "X_scaled_data['Entropy_DU'] = agg_data['Entropy_DU'].values\n",
        "X_scaled_data.fillna(0, inplace=True)\n",
        "\n",
        "# Balance DUs with SMOTE and propagate DU_ID\n",
        "X_array = X_scaled_data.drop(columns=['DU_ID', 'Label', 'Entropy_DU']).values\n",
        "y_array = X_scaled_data['Label'].values\n",
        "smote = SMOTE(random_state=42, k_neighbors=5)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_array, y_array)\n",
        "\n",
        "# Find nearest neighbors to assign DU_ID to synthetic samples\n",
        "nn = NearestNeighbors(n_neighbors=1).fit(X_array)\n",
        "distances, indices = nn.kneighbors(X_resampled)\n",
        "du_ids_resampled = X_scaled_data['DU_ID'].iloc[indices.flatten()].values\n",
        "entropy_du_resampled = X_scaled_data['Entropy_DU'].iloc[indices.flatten()].values\n",
        "\n",
        "X_scaled_data_resampled = pd.DataFrame(X_resampled, columns=X_scaled_data.drop(columns=['DU_ID', 'Label', 'Entropy_DU']).columns)\n",
        "X_scaled_data_resampled['Label'] = y_resampled\n",
        "X_scaled_data_resampled['DU_ID'] = du_ids_resampled\n",
        "X_scaled_data_resampled['Entropy_DU'] = entropy_du_resampled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NUnTxqJoGFB",
        "outputId": "137e919c-0522-429c-bcad-c6f3d85ffd52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-3086762825>:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['Protocol'].fillna(data['Protocol'].mode()[0], inplace=True)\n",
            "<ipython-input-1-3086762825>:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['Flags'].fillna(data['Flags'].mode()[0], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated DU Duration: 2 seconds\n",
            "Packets per DU:\n",
            " count    733.000000\n",
            "mean      47.553888\n",
            "std      131.489508\n",
            "min        2.000000\n",
            "25%        8.000000\n",
            "50%       14.000000\n",
            "75%       24.000000\n",
            "max      733.000000\n",
            "dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-3086762825>:98: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  agg_data['Entropy_DU'] = data.groupby('DU_ID').apply(compute_du_entropy).reindex(agg_data.index).values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AKN: Calculate Neurons' Number (Algorithm 1)\n",
        "def calculate_optimal_neurons(X, threshold_ratio=0.5):\n",
        "    centers = [X[0]]\n",
        "    max_dist = 0\n",
        "    second = X[1]\n",
        "    for row in X:\n",
        "        dist = euclidean(row, centers[0])\n",
        "        if dist > max_dist:\n",
        "            max_dist = dist\n",
        "            second = row\n",
        "    centers.append(second)\n",
        "    while True:\n",
        "        min_dists = [min([euclidean(row, center) for center in centers]) for row in X]\n",
        "        D_max = max(min_dists)\n",
        "        if D_max > threshold_ratio * euclidean(centers[0], centers[1]):\n",
        "            new_center = X[np.argmax(min_dists)]\n",
        "            centers.append(new_center)\n",
        "        else:\n",
        "            break\n",
        "    return len(centers), np.array(centers)\n",
        "\n",
        "# AKN: Calculate Initial Weights (Algorithm 2)\n",
        "def calculate_initial_weights(R, numcl):\n",
        "    N = (R - R.min(axis=0)) / (R.max(axis=0) - R.min(axis=0) + 1e-10)\n",
        "    C = N.mean(axis=0)\n",
        "    dmax = np.max([euclidean(n, C) for n in N])\n",
        "    X = np.zeros((R.shape[1], numcl))\n",
        "    for j in range(numcl):\n",
        "        X[:, j] = C + np.random.uniform(-dmax, dmax, R.shape[1])\n",
        "    return X.T\n",
        "\n",
        "# AKN: Clustering (Algorithm 3)\n",
        "def kohonen_clustering(X, initial_centers, num_epochs=100, initial_lr=0.5, initial_radius=0.5):\n",
        "    weights = initial_centers.copy()\n",
        "    num_neurons = len(weights)\n",
        "    sigma = initial_radius * num_neurons\n",
        "    sigma_decay = sigma / num_epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        lr = initial_lr * np.exp(-epoch / num_epochs)\n",
        "        sigma = sigma - sigma_decay * epoch\n",
        "        for x in X:\n",
        "            distances = euclidean_distances([x], weights).flatten()\n",
        "            winner_idx = np.argmin(distances)\n",
        "            for j in range(num_neurons):\n",
        "                dist_to_winner = np.abs(j - winner_idx)\n",
        "                influence = np.exp(-dist_to_winner**2 / (2 * sigma**2))\n",
        "                weights[j] += lr * influence * (x - weights[j])\n",
        "    assignments = [np.argmin(euclidean_distances([x], weights)) for x in X]\n",
        "    return assignments, weights\n"
      ],
      "metadata": {
        "id": "9_I89CAMIj0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply AKN\n",
        "X_clustering = X_scaled_data_resampled.drop(columns=['Label', 'DU_ID', 'Entropy_DU']).values\n",
        "num_neurons, initial_centers = calculate_optimal_neurons(X_clustering, threshold_ratio=0.5)\n",
        "initial_weights = calculate_initial_weights(X_clustering, num_neurons)\n",
        "cluster_labels, final_weights = kohonen_clustering(X_clustering, initial_weights, num_epochs=100)\n",
        "X_scaled_data_resampled['Cluster'] = cluster_labels\n",
        "print(\"Clustering Silhouette Score:\", silhouette_score(X_clustering, cluster_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poZO5bU9InPP",
        "outputId": "67fff3f6-f7b3-41bd-cb13-a3e977ed15a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clustering Silhouette Score: 0.7575332822833021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Compute MAD\n",
        "mean_C = (X_scaled_data_resampled['TCP_Packets_mean'] / (X_scaled_data_resampled['TCP_Packets_mean'] + 1e-10)).mean()\n",
        "std_C = (X_scaled_data_resampled['TCP_Packets_mean'] / (X_scaled_data_resampled['TCP_Packets_mean'] + 1e-10)).std()\n",
        "mean_H = X_scaled_data_resampled['Entropy_DU'].mean()\n",
        "std_H = X_scaled_data_resampled['Entropy_DU'].std()\n",
        "# Compute Pearson correlation per DU\n",
        "pearson_coeffs = []\n",
        "for du_id in X_scaled_data_resampled['DU_ID'].unique():\n",
        "    du_data = data[data['DU_ID'] == du_id]\n",
        "    if len(du_data) > 1 and du_data['Total_Packets'].sum() > 0:\n",
        "        corr, _ = pearsonr(du_data['TCP_Packets'], du_data['Total_Packets'])\n",
        "        pearson_coeffs.append(corr)\n",
        "    else:\n",
        "        pearson_coeffs.append(0)\n",
        "pearson_map = dict(zip(X_scaled_data_resampled['DU_ID'].unique(), pearson_coeffs))\n",
        "X_scaled_data_resampled['Pearson_Corr'] = X_scaled_data_resampled['DU_ID'].map(pearson_map)\n",
        "mean_P = X_scaled_data_resampled['Pearson_Corr'].mean()\n",
        "std_P = X_scaled_data_resampled['Pearson_Corr'].std()\n",
        "\n",
        "mad_scores = []\n",
        "for _, row in X_scaled_data_resampled.iterrows():\n",
        "    C = (row['TCP_Packets_mean'] / (row['TCP_Packets_mean'] + 1e-10) - mean_C) / (std_C + 1e-10)\n",
        "    H = (row['Entropy_DU'] - mean_H) / (std_H + 1e-10)\n",
        "    P = (row['Pearson_Corr'] - mean_P) / (std_P + 1e-10) if std_P > 1e-10 else 0\n",
        "    mad = np.sqrt(0.4 * C**2 + 0.3 * H**2 + 0.3 * P**2)\n",
        "    mad_scores.append(mad)\n",
        "X_scaled_data_resampled['MAD'] = mad_scores\n",
        "X_scaled_data_resampled['MAD'] = scaler.fit_transform(X_scaled_data_resampled[['MAD']]).flatten()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ4MSszEIsVi",
        "outputId": "92b0dadd-67a7-46d3-e69a-deef2a151eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-940486787>:11: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  corr, _ = pearsonr(du_data['TCP_Packets'], du_data['Total_Packets'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimize Thresholds\n",
        "fpr, tpr, thresholds = roc_curve(X_scaled_data_resampled['Label'], X_scaled_data_resampled['MAD'])\n",
        "optimal_idx = np.argmax(tpr - fpr)\n",
        "optimal_z = (thresholds[optimal_idx] - X_scaled_data_resampled['MAD'].mean()) / X_scaled_data_resampled['MAD'].std()\n",
        "LMAD = X_scaled_data_resampled['MAD'].mean() + optimal_z * X_scaled_data_resampled['MAD'].std()\n",
        "X_scaled_data_resampled['Abnormal_DU'] = (X_scaled_data_resampled['MAD'] > LMAD).astype(int)\n",
        "\n",
        "best_ladur = 0.55\n",
        "best_f1 = 0\n",
        "for ladur in [0.5, 0.55, 0.6]:\n",
        "    cluster_votes = X_scaled_data_resampled.groupby('Cluster')['Abnormal_DU'].mean()\n",
        "    abnormal_clusters = cluster_votes[cluster_votes >= ladur].index\n",
        "    X_scaled_data_resampled['Predicted_Label'] = X_scaled_data_resampled['Cluster'].apply(lambda c: 1 if c in abnormal_clusters else 0)\n",
        "    f1 = f1_score(X_scaled_data_resampled['Label'], X_scaled_data_resampled['Predicted_Label'])\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_ladur = ladur"
      ],
      "metadata": {
        "id": "lqPlOJ0dItA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Detection\n",
        "X_scaled_data_resampled['Abnormal_DU'] = (X_scaled_data_resampled['MAD'] > LMAD).astype(int)\n",
        "cluster_votes = X_scaled_data_resampled.groupby('Cluster')['Abnormal_DU'].mean()\n",
        "abnormal_clusters = cluster_votes[cluster_votes >= best_ladur].index\n",
        "X_scaled_data_resampled['Predicted_Label'] = X_scaled_data_resampled['Cluster'].apply(lambda c: 1 if c in abnormal_clusters else 0)\n",
        "\n",
        "# Impute any remaining NaN in Predicted_Label\n",
        "X_scaled_data_resampled['Predicted_Label'] = X_scaled_data_resampled['Predicted_Label'].fillna(0).astype(int)\n",
        "\n",
        "print(\"Final Detection Evaluation:\\n\")\n",
        "print(classification_report(X_scaled_data_resampled['Label'], X_scaled_data_resampled['Predicted_Label']))\n",
        "\n",
        "# Cross-validate Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "# scores = cross_val_score(rf, X_array, X_scaled_data_resampled['Label'], cv=5, scoring='f1_weighted')\n",
        "scores = cross_val_score(rf, X_resampled, y_resampled, cv=5, scoring='f1_weighted')\n",
        "print(\"Random Forest Cross-Validation F1 Scores:\", scores, \"Mean:\", scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "BNh_Xh2lIwn3",
        "outputId": "d56c189f-c123-40cf-e2f2-48a84114c287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'MAD'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'MAD'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-1324512805>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Final Detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_scaled_data_resampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Abnormal_DU'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_scaled_data_resampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MAD'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mLMAD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcluster_votes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_scaled_data_resampled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cluster'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Abnormal_DU'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mabnormal_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_votes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcluster_votes\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mbest_ladur\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_scaled_data_resampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted_Label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_scaled_data_resampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Cluster'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mabnormal_clusters\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'MAD'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pywt\n",
        "from sklearn.metrics import classification_report, roc_curve, f1_score, silhouette_score\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from scipy.spatial.distance import euclidean\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import pearsonr\n",
        "from collections import Counter\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Load dataset\n",
        "URL = '/content/drive/MyDrive/Colab Notebooks/Client1 (copy).csv'\n",
        "data = pd.read_csv(URL)\n",
        "\n",
        "# Impute missing values\n",
        "data.fillna(data.median(numeric_only=True), inplace=True)\n",
        "data['Protocol'].fillna(data['Protocol'].mode()[0], inplace=True)\n",
        "data['Flags'].fillna(data['Flags'].mode()[0], inplace=True)\n",
        "\n",
        "# Convert types and encode categorical features\n",
        "data = data.astype({'Time': 'float', 'Label': int})\n",
        "data = pd.get_dummies(data, columns=['Protocol', 'Flags'], prefix=['Protocol', 'Flags'])\n",
        "data = data.drop(columns=['Source', 'Destination'], errors='ignore')\n",
        "\n",
        "# Sequence Alignment with Smith-Waterman Algorithm\n",
        "def smith_waterman(seq1, seq2, match_score=2, mismatch_score=-2, gap_penalty=-1):\n",
        "    n, m = len(seq1), len(seq2)\n",
        "    H = np.zeros((n+1, m+1))\n",
        "    for i in range(1, n+1):\n",
        "        for j in range(1, m+1):\n",
        "            match = H[i-1, j-1] + (match_score if seq1[i-1] == seq2[j-1] else mismatch_score)\n",
        "            delete = H[i-1, j] + gap_penalty\n",
        "            insert = H[i, j-1] + gap_penalty\n",
        "            H[i, j] = max(match, delete, insert, 0)\n",
        "    return np.max(H)\n",
        "\n",
        "# Construct benchmark sequences for RTO values\n",
        "rto_values = [1, 2]\n",
        "benchmark_sequences = {rto: np.array([1 if i % rto < 0.1 else 0 for i in np.arange(0, 60, 0.1)]) for rto in rto_values}\n",
        "\n",
        "# Estimate DU duration using UDP packet counts\n",
        "cw_duration = 60\n",
        "data['CW_ID'] = ((data['Time'] - data['Time'].min()) // cw_duration).astype(int)\n",
        "udp_data = data[data['Protocol_UDP'] == 1]\n",
        "cw_udp_counts = udp_data.groupby('CW_ID')['Length'].count().reindex(data['CW_ID'].unique(), fill_value=0)\n",
        "cw_udp_seq = cw_udp_counts.values\n",
        "\n",
        "max_score = -np.inf\n",
        "time_du = 1\n",
        "for rto, seq in benchmark_sequences.items():\n",
        "    score = smith_waterman(cw_udp_seq[:len(seq)], seq)\n",
        "    if score > max_score:\n",
        "        max_score = score\n",
        "        time_du = rto\n",
        "\n",
        "# Create DUs based on estimated TimeDU\n",
        "data['DU_ID'] = ((data['Time'] - data['Time'].min()) // time_du).astype(int)\n",
        "print(\"Estimated DU Duration:\", time_du, \"seconds\")\n",
        "print(\"Packets per DU:\\n\", data.groupby('DU_ID').size().describe())\n",
        "\n",
        "# Feature Aggregation\n",
        "data['TCP_Packets'] = data['Protocol_TCP']\n",
        "data['Total_Packets'] = 1\n",
        "data['Burstiness'] = data.groupby('DU_ID')['Packet_Rate'].transform(lambda x: x.max() / (x.mean() + 1e-10))\n",
        "\n",
        "features_to_aggregate = [\n",
        "    'Length', 'Inter_Arrival_Time', 'Connection_Duration', 'Packet_Rate',\n",
        "    'Flow_Bytes_Per_Second', 'Flow_Packets_Per_Second', 'Forward_Packets',\n",
        "    'Backward_Packets', 'Ratio_Fwd_Bwd', 'Entropy', 'Packet_Size_Variance',\n",
        "    'Burstiness', 'TCP_Packets', 'Total_Packets'\n",
        "]\n",
        "\n",
        "agg_funcs = {col: ['mean', 'std', 'max', 'min'] for col in features_to_aggregate}\n",
        "agg_funcs['Label'] = lambda x: 1 if (x == 1).mean() >= 0.05 else 0\n",
        "agg_data = data.groupby('DU_ID').agg(agg_funcs)\n",
        "agg_data.columns = ['_'.join(col) if isinstance(col, tuple) else col for col in agg_data.columns]\n",
        "agg_data.reset_index(inplace=True)\n",
        "\n",
        "# Compute wavelet packet entropy per DU\n",
        "def compute_du_entropy(group):\n",
        "    signal = group['Length'].values\n",
        "    if len(signal) < 2:\n",
        "        return 0\n",
        "    wp = pywt.WaveletPacket(data=signal, wavelet='db1', mode='symmetric', maxlevel=3)\n",
        "    energies = [np.sum(np.square(node.data)) for node in wp.get_level(3)]\n",
        "    total_energy = np.sum(energies)\n",
        "    probs = np.array(energies) / (total_energy + 1e-10)\n",
        "    return -np.sum(probs * np.log2(probs + 1e-10))\n",
        "\n",
        "agg_data['Entropy_DU'] = data.groupby('DU_ID').apply(compute_du_entropy).reindex(agg_data.index).values\n",
        "\n",
        "# Feature Scaling\n",
        "agg_data_features = agg_data.drop(columns=['DU_ID', 'Label_<lambda>'])\n",
        "agg_data_features.fillna(0, inplace=True)\n",
        "\n",
        "for col in agg_data_features.columns:\n",
        "    if agg_data_features[col].dtype == 'bool':\n",
        "        agg_data_features[col] = agg_data_features[col].astype(int)\n",
        "\n",
        "for col in agg_data_features.columns:\n",
        "    p1, p99 = agg_data_features[col].quantile([0.01, 0.99])\n",
        "    agg_data_features[col] = agg_data_features[col].clip(p1, p99)\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(agg_data_features)\n",
        "X_scaled_data = pd.DataFrame(X_scaled, columns=agg_data_features.columns)\n",
        "X_scaled_data['DU_ID'] = agg_data['DU_ID'].values\n",
        "X_scaled_data['Label'] = agg_data['Label_<lambda>'].values\n",
        "X_scaled_data['Entropy_DU'] = agg_data['Entropy_DU'].values\n",
        "X_scaled_data.fillna(0, inplace=True)\n",
        "\n",
        "# Compute Pearson correlation per DU\n",
        "pearson_coeffs = []\n",
        "for du_id in X_scaled_data['DU_ID'].unique():\n",
        "    du_data = data[data['DU_ID'] == du_id]\n",
        "    if len(du_data) > 1 and du_data['Total_Packets'].sum() > 0:\n",
        "        corr, _ = pearsonr(du_data['TCP_Packets'], du_data['Total_Packets'])\n",
        "        pearson_coeffs.append(corr if not np.isnan(corr) else 0)\n",
        "    else:\n",
        "        pearson_coeffs.append(0)\n",
        "pearson_map = dict(zip(X_scaled_data['DU_ID'].unique(), pearson_coeffs))\n",
        "X_scaled_data['Pearson_Corr'] = X_scaled_data['DU_ID'].map(pearson_map)\n",
        "\n",
        "# AKN: Calculate Neurons' Number (Algorithm 1)\n",
        "def calculate_optimal_neurons(X, threshold_ratio=0.5):\n",
        "    centers = [X[0]]\n",
        "    max_dist = 0\n",
        "    second = X[1]\n",
        "    for row in X:\n",
        "        dist = euclidean(row, centers[0])\n",
        "        if dist > max_dist:\n",
        "            max_dist = dist\n",
        "            second = row\n",
        "    centers.append(second)\n",
        "    while True:\n",
        "        min_dists = [min([euclidean(row, center) for center in centers]) for row in X]\n",
        "        D_max = max(min_dists)\n",
        "        if D_max > threshold_ratio * euclidean(centers[0], centers[1]):\n",
        "            new_center = X[np.argmax(min_dists)]\n",
        "            centers.append(new_center)\n",
        "        else:\n",
        "            break\n",
        "    return len(centers), np.array(centers)\n",
        "\n",
        "# AKN: Calculate Initial Weights (Algorithm 2)\n",
        "def calculate_initial_weights(R, numcl):\n",
        "    N = (R - R.min(axis=0)) / (R.max(axis=0) - R.min(axis=0) + 1e-10)\n",
        "    C = N.mean(axis=0)\n",
        "    dmax = np.max([euclidean(n, C) for n in N])\n",
        "    X = np.zeros((R.shape[1], numcl))\n",
        "    for j in range(numcl):\n",
        "        X[:, j] = C + np.random.uniform(-dmax, dmax, R.shape[1])\n",
        "    return X.T\n",
        "\n",
        "# AKN: Clustering (Algorithm 3)\n",
        "def kohonen_clustering(X, initial_centers, num_epochs=100, initial_lr=0.5, initial_radius=0.5):\n",
        "    weights = initial_centers.copy()\n",
        "    num_neurons = len(weights)\n",
        "    sigma = initial_radius * num_neurons\n",
        "    sigma_decay = sigma / num_epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        lr = initial_lr * np.exp(-epoch / num_epochs)\n",
        "        sigma = sigma - sigma_decay * epoch\n",
        "        for x in X:\n",
        "            distances = euclidean_distances([x], weights).flatten()\n",
        "            winner_idx = np.argmin(distances)\n",
        "            for j in range(num_neurons):\n",
        "                dist_to_winner = np.abs(j - winner_idx)\n",
        "                influence = np.exp(-dist_to_winner**2 / (2 * sigma**2))\n",
        "                weights[j] += lr * influence * (x - weights[j])\n",
        "    assignments = [np.argmin(euclidean_distances([x], weights)) for x in X]\n",
        "    return assignments, weights\n",
        "\n",
        "# Apply AKN on original data\n",
        "X_clustering = X_scaled_data.drop(columns=['DU_ID', 'Label', 'Entropy_DU', 'Pearson_Corr']).values\n",
        "num_neurons, initial_centers = calculate_optimal_neurons(X_clustering, threshold_ratio=0.5)\n",
        "initial_weights = calculate_initial_weights(X_clustering, num_neurons)\n",
        "cluster_labels, final_weights = kohonen_clustering(X_clustering, initial_weights, num_epochs=100)\n",
        "X_scaled_data['Cluster'] = cluster_labels\n",
        "print(\"Clustering Silhouette Score:\", silhouette_score(X_clustering, cluster_labels))\n",
        "\n",
        "# Compute MAD on original data\n",
        "mean_C = (X_scaled_data['TCP_Packets_mean'] / (X_scaled_data['TCP_Packets_mean'] + 1e-10)).mean()\n",
        "std_C = (X_scaled_data['TCP_Packets_mean'] / (X_scaled_data['TCP_Packets_mean'] + 1e-10)).std() or 1e-10\n",
        "mean_H = X_scaled_data['Entropy_DU'].mean()\n",
        "std_H = X_scaled_data['Entropy_DU'].std() or 1e-10\n",
        "mean_P = X_scaled_data['Pearson_Corr'].mean()\n",
        "std_P = X_scaled_data['Pearson_Corr'].std() or 1e-10\n",
        "\n",
        "mad_scores = []\n",
        "for _, row in X_scaled_data.iterrows():\n",
        "    C = (row['TCP_Packets_mean'] / (row['TCP_Packets_mean'] + 1e-10) - mean_C) / std_C\n",
        "    H = (row['Entropy_DU'] - mean_H) / std_H\n",
        "    P = (row['Pearson_Corr'] - mean_P) / std_P\n",
        "    mad = np.sqrt(0.4 * C**2 + 0.3 * H**2 + 0.3 * P**2)\n",
        "    mad_scores.append(mad if not np.isnan(mad) else 0)\n",
        "X_scaled_data['MAD'] = mad_scores\n",
        "X_scaled_data['MAD'] = scaler.fit_transform(X_scaled_data[['MAD']]).flatten()\n",
        "X_scaled_data['MAD'].fillna(0, inplace=True)\n",
        "\n",
        "# Balance data for threshold optimization\n",
        "X_array = X_scaled_data.drop(columns=['DU_ID', 'Label', 'Entropy_DU', 'Pearson_Corr', 'Cluster', 'MAD']).values\n",
        "y_array = X_scaled_data['Label'].values\n",
        "smote = SMOTE(random_state=42, k_neighbors=5)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_array, y_array)\n",
        "X_scaled_data_resampled = pd.DataFrame(X_resampled, columns=X_scaled_data.drop(columns=['DU_ID', 'Label', 'Entropy_DU', 'Pearson_Corr', 'Cluster', 'MAD']).columns)\n",
        "X_scaled_data_resampled['Label'] = y_resampled\n",
        "X_scaled_data_resampled['MAD'] = scaler.transform(X_scaled_data[['MAD']].iloc[:len(X_resampled)].values).flatten()\n",
        "\n",
        "# Optimize Thresholds\n",
        "fpr, tpr, thresholds = roc_curve(X_scaled_data_resampled['Label'], X_scaled_data_resampled['MAD'])\n",
        "optimal_idx = np.argmax(tpr - fpr)\n",
        "optimal_z = (thresholds[optimal_idx] - X_scaled_data_resampled['MAD'].mean()) / X_scaled_data_resampled['MAD'].std()\n",
        "LMAD = X_scaled_data_resampled['MAD'].mean() + optimal_z * X_scaled_data_resampled['MAD'].std()\n",
        "X_scaled_data['Abnormal_DU'] = (X_scaled_data['MAD'] > LMAD).astype(int)\n",
        "\n",
        "best_ladur = 0.55\n",
        "best_f1 = 0\n",
        "for ladur in [0.5, 0.55, 0.6]:\n",
        "    cluster_votes = X_scaled_data.groupby('Cluster')['Abnormal_DU'].mean()\n",
        "    abnormal_clusters = cluster_votes[cluster_votes >= ladur].index\n",
        "    X_scaled_data['Predicted_Label'] = X_scaled_data['Cluster'].apply(lambda c: 1 if c in abnormal_clusters else 0)\n",
        "    f1 = f1_score(X_scaled_data['Label'], X_scaled_data['Predicted_Label'])\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_ladur = ladur\n",
        "\n",
        "# Final Detection\n",
        "LMAD = X_scaled_data['MAD'].mean() + optimal_z * X_scaled_data['MAD'].std()\n",
        "X_scaled_data['Abnormal_DU'] = (X_scaled_data['MAD'] > LMAD).astype(int)\n",
        "cluster_votes = X_scaled_data.groupby('Cluster')['Abnormal_DU'].mean()\n",
        "abnormal_clusters = cluster_votes[cluster_votes >= best_ladur].index\n",
        "X_scaled_data['Predicted_Label'] = X_scaled_data['Cluster'].apply(lambda c: 1 if c in abnormal_clusters else 0)\n",
        "X_scaled_data['Predicted_Label'].fillna(0, inplace=True)  # Fallback for any remaining NaN\n",
        "print(\"Final Detection Evaluation:\\n\")\n",
        "print(classification_report(X_scaled_data['Label'], X_scaled_data['Predicted_Label']))\n",
        "\n",
        "# Cross-validate Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "scores = cross_val_score(rf, X_array, X_scaled_data['Label'], cv=5, scoring='f1_weighted')\n",
        "print(\"Random Forest Cross-Validation F1 Scores:\", scores, \"Mean:\", scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FJ1UT1oNKuse",
        "outputId": "01d94ef6-e58d-47c9-c3fc-c5bd3b65c8e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-3979325510>:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['Protocol'].fillna(data['Protocol'].mode()[0], inplace=True)\n",
            "<ipython-input-21-3979325510>:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['Flags'].fillna(data['Flags'].mode()[0], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated DU Duration: 2 seconds\n",
            "Packets per DU:\n",
            " count    733.000000\n",
            "mean      47.553888\n",
            "std      131.489508\n",
            "min        2.000000\n",
            "25%        8.000000\n",
            "50%       14.000000\n",
            "75%       24.000000\n",
            "max      733.000000\n",
            "dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-3979325510>:98: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  agg_data['Entropy_DU'] = data.groupby('DU_ID').apply(compute_du_entropy).reindex(agg_data.index).values\n",
            "<ipython-input-21-3979325510>:125: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  corr, _ = pearsonr(du_data['TCP_Packets'], du_data['Total_Packets'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clustering Silhouette Score: -0.5314425762953277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-3979325510>:207: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_scaled_data['MAD'].fillna(0, inplace=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RobustScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Length of values (733) does not match length of index (1390)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-3979325510>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0mX_scaled_data_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_scaled_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DU_ID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Entropy_DU'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Pearson_Corr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Cluster'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MAD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0mX_scaled_data_resampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_resampled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m \u001b[0mX_scaled_data_resampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MAD'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_scaled_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MAD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_resampled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;31m# Optimize Thresholds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4309\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4310\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4311\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4522\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4523\u001b[0m         \"\"\"\n\u001b[0;32m-> 4524\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4526\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5266\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5267\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5268\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \"\"\"\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    574\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length of values (733) does not match length of index (1390)"
          ]
        }
      ]
    }
  ]
}